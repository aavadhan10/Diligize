'upload_time': datetime.now(),
                    'word_count': 245,
                    'char_count': 1456
                }
            }
            
            st.session_state.uploaded_files.update(sample_docs)
            st.success("‚úÖ Sample documents loaded! You can now proceed to AI Analysis.")
            st.rerun()

elif workflow_step == "ü§ñ AI Analysis":
    st.header("ü§ñ AI-Powered Document Analysis")
    
    if not st.session_state.uploaded_files:
        st.warning("‚ö†Ô∏è Please upload documents first in the Document Upload step.")
        st.info("üëà Use the sidebar to navigate back to Document Upload")
    else:
        st.info("üß† **Claude AI** will analyze each document to extract structured data and identify compliance items.")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("Document Processing Status")
            
            # Show analysis button with enhanced styling
            analysis_button = st.button(
                "üöÄ Start Comprehensive AI Analysis", 
                type="primary", 
                disabled=not st.session_state.uploaded_files,
                help="Begin AI analysis of all uploaded documents"
            )
            
            if analysis_button:
                # Enhanced progress tracking
                st.markdown("### üîÑ Analysis in Progress...")
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                analysis_details = st.empty()
                
                total_files = len(st.session_state.uploaded_files)
                
                for i, (filename, doc_info) in enumerate(st.session_state.uploaded_files.items()):
                    # Update progress display
                    progress = (i + 1) / total_files
                    progress_bar.progress(progress)
                    status_text.markdown(f"**Analyzing:** `{filename}` ({doc_info['type']})")
                    
                    with analysis_details.container():
                        st.write(f"üìÑ **File {i+1}/{total_files}:** {filename}")
                        st.write(f"üìã **Type:** {doc_info['type']}")
                        st.write(f"üìä **Size:** {doc_info['char_count']:,} characters")
                        
                        # Show processing steps
                        with st.spinner("Processing with Claude AI..."):
                            analysis = analyze_document_with_ai(
                                doc_info['content'], 
                                filename, 
                                doc_info['type']
                            )
                        
                        st.session_state.analysis_results[filename] = analysis
                        
                        # Show immediate results
                        if analysis['confidence_score'] > 0:
                            st.success(f"‚úÖ Analysis complete - Confidence: {analysis['confidence_score']*100:.1f}%")
                            if analysis['issues_found']:
                                st.warning(f"‚ö†Ô∏è {len(analysis['issues_found'])} potential issues identified")
                        else:
                            st.error("‚ùå Analysis failed - Check API configuration")
                        
                        st.markdown("---")
                
                status_text.markdown("### ‚úÖ **Analysis Complete!**")
                st.balloons()
                st.success(f"üéâ Successfully analyzed {total_files} documents with AI!")
        
        with col2:
            st.subheader("üìä Analysis Dashboard")
            
            if st.session_state.analysis_results:
                # Real-time analysis metrics
                analyzed_count = len(st.session_state.analysis_results)
                total_count = len(st.session_state.uploaded_files)
                
                st.metric("üìÑ Documents Processed", f"{analyzed_count}/{total_count}")
                
                # Calculate detailed metrics
                confidence_scores = [doc.get('confidence_score', 0) for doc in st.session_state.analysis_results.values()]
                avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
                
                st.metric("üéØ Average Confidence", f"{avg_confidence*100:.1f}%")
                
                # Issue tracking
                total_issues = sum(len(doc.get('issues_found', [])) for doc in st.session_state.analysis_results.values())
                st.metric("‚ö†Ô∏è Issues Identified", total_issues)
                
                # Compliance items found
                total_compliance = sum(len(doc.get('compliance_items', [])) for doc in st.session_state.analysis_results.values())
                st.metric("‚úÖ Compliance Items", total_compliance)
                
                # Analysis quality chart
                if confidence_scores:
                    fig = px.bar(
                        x=list(st.session_state.analysis_results.keys()),
                        y=[score*100 for score in confidence_scores],
                        title="Analysis Confidence by Document",
                        labels={'x': 'Document', 'y': 'Confidence %'}
                    )
                    fig.update_layout(xaxis_tickangle=-45, height=300)
                    st.plotly_chart(fig, use_container_width=True)
            
            else:
                st.info("üìä Analysis results will appear here")
                
                # Show what will be analyzed
                st.subheader("üìã Ready to Analyze")
                for filename, doc_info in st.session_state.uploaded_files.items():
                    st.write(f"‚Ä¢ **{filename}** ({doc_info['type']})")
        
        # Enhanced results display
        if st.session_state.analysis_results:
            st.markdown("---")
            st.subheader("üìã Detailed Analysis Results")
            
            # Results summary tabs
            tab1, tab2, tab3 = st.tabs(["üìä Summary", "üìÑ By Document", "‚ö†Ô∏è Issues Found"])
            
            with tab1:
                # Comprehensive summary
                st.markdown("### üìà Analysis Summary")
                
                # Create summary metrics
                summary_data = {
                    'Document Type': [],
                    'Count': [],
                    'Avg Confidence': [],
                    'Issues Found': [],
                    'Key Data Extracted': []
                }
                
                doc_type_groups = {}
                for filename, analysis in st.session_state.analysis_results.items():
                    doc_type = analysis['document_type']
                    if doc_type not in doc_type_groups:
                        doc_type_groups[doc_type] = []
                    doc_type_groups[doc_type].append(analysis)
                
                for doc_type, analyses in doc_type_groups.items():
                    summary_data['Document Type'].append(doc_type)
                    summary_data['Count'].append(len(analyses))
                    
                    confidences = [a['confidence_score'] for a in analyses]
                    summary_data['Avg Confidence'].append(f"{np.mean(confidences)*100:.1f}%")
                    
                    total_issues = sum(len(a['issues_found']) for a in analyses)
                    summary_data['Issues Found'].append(total_issues)
                    
                    # Count extracted data points
                    data_points = sum(len(a.get('extracted_data', {})) for a in analyses)
                    summary_data['Key Data Extracted'].append(data_points)
                
                summary_df = pd.DataFrame(summary_data)
                st.dataframe(summary_df, use_container_width=True)
            
            with tab2:
                # Individual document results
                for filename, analysis in st.session_state.analysis_results.items():
                    with st.expander(f"üìÑ {filename} - {analysis['document_type']}", expanded=False):
                        
                        # Document analysis header
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Confidence", f"{analysis['confidence_score']*100:.1f}%")
                        with col2:
                            st.metric("Data Points", len(analysis.get('extracted_data', {})))
                        with col3:
                            st.metric("Issues", len(analysis['issues_found']))
                        
                        # Extracted data display
                        st.markdown("**üîç Extracted Data:**")
                        extracted_data = analysis.get('extracted_data', {})
                        
                        if extracted_data and not extracted_data.get('api_error'):
                            # Format data in a nice table
                            if isinstance(extracted_data, dict):
                                data_rows = []
                                for key, value in extracted_data.items():
                                    if key not in ['compliance_items', 'issues_found', 'raw_response']:
                                        if isinstance(value, (dict, list)):
                                            value_str = json.dumps(value, indent=2) if len(str(value)) < 200 else str(value)[:200] + "..."
                                        else:
                                            value_str = str(value)
                                        data_rows.append({
                                            'Field': key.replace('_', ' ').title(),
                                            'Value': value_str
                                        })
                                
                                if data_rows:
                                    data_df = pd.DataFrame(data_rows)
                                    st.dataframe(data_df, use_container_width=True)
                                else:
                                    st.info("No structured data extracted")
                            else:
                                st.code(json.dumps(extracted_data, indent=2))
                        else:
                            st.warning("No data extracted or API error occurred")
                        
                        # Compliance and issues
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**‚úÖ Compliance Items Found:**")
                            compliance_items = analysis.get('compliance_items', [])
                            if compliance_items:
                                for item in compliance_items:
                                    st.write(f"‚Ä¢ {item}")
                            else:
                                st.info("No compliance items identified")
                        
                        with col2:
                            st.markdown("**‚ö†Ô∏è Issues Identified:**")
                            issues = analysis.get('issues_found', [])
                            if issues:
                                for issue in issues:
                                    st.warning(f"‚Ä¢ {issue}")
                            else:
                                st.success("‚Ä¢ No issues found")
                        
                        # Raw response (for debugging)
                        if st.checkbox(f"Show AI Response for {filename}", key=f"raw_{filename}"):
                            st.markdown("**ü§ñ Raw AI Response:**")
                            st.code(analysis.get('raw_response', 'No response available'))
            
            with tab3:
                # Consolidated issues view
                all_issues = []
                for filename, analysis in st.session_state.analysis_results.items():
                    for issue in analysis.get('issues_found', []):
                        all_issues.append({
                            'Document': filename,
                            'Type': analysis['document_type'],
                            'Issue': issue,
                            'Severity': 'High' if 'error' in issue.lower() or 'missing' in issue.lower() else 'Medium'
                        })
                
                if all_issues:
                    issues_df = pd.DataFrame(all_issues)
                    st.dataframe(issues_df, use_container_width=True)
                    
                    # Issues by severity
                    severity_counts = issues_df['Severity'].value_counts()
                    fig = px.pie(
                        values=severity_counts.values,
                        names=severity_counts.index,
                        title="Issues by Severity",
                        color_discrete_map={'High': 'red', 'Medium': 'orange', 'Low': 'yellow'}
                    )
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.success("üéâ No issues found in any documents!")

elif workflow_step == "‚úÖ Compliance Check":
    st.header("‚úÖ Due Diligence Compliance Verification")
    
    if not st.session_state.analysis_results:
        st.warning("‚ö†Ô∏è Please complete AI analysis first.")
        st.info("üëà Use the sidebar to navigate back to AI Analysis")
    else:
        st.info("üîç **Automated compliance checking** against comprehensive due diligence requirements")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üìã Comprehensive Due Diligence Checklist")
            
            # Enhanced compliance check button
            compliance_button = st.button(
                "üîç Run Complete Compliance Analysis", 
                type="primary",
                help="Analyze all documents against due diligence requirements"
            )
            
            if compliance_button:
                st.markdown("### üîÑ Compliance Check in Progress...")
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                total_items = sum(len(items) for section in DD_CHECKLIST.values() for items in section.values())
                current_item = 0
                
                # Process each compliance item
                for section_name, subsections in DD_CHECKLIST.items():
                    for subsection_name, items in subsections.items():
                        for item in items:
                            current_item += 1
                            progress = current_item / total_items
                            
                            status_text.text(f"Checking: {item[:60]}..." if len(item) > 60 else f"Checking: {item}")
                            progress_bar.progress(progress)
                            
                            # Perform actual compliance check
                            compliance_result = check_compliance_item(item, st.session_state.analysis_results)
                            st.session_state.checklist_status[item] = compliance_result
                            
                            # Add failed items to discrepancies
                            if compliance_result[0] is False:
                                discrepancy = {
                                    'type': 'Compliance Failure',
                                    'description': item,
                                    'severity': 'High',
                                    'recommendation': compliance_result[1],
                                    'section': section_name,
                                    'subsection': subsection_name
                                }
                                
                                # Avoid duplicates
                                if not any(d['description'] == item for d in st.session_state.discrepancies):
                                    st.session_state.discrepancies.append(discrepancy)
                
                status_text.text("‚úÖ Compliance analysis complete!")
                st.success("üéâ Due diligence compliance check completed!")
                st.balloons()
            
            # Display comprehensive results
            if st.session_state.checklist_status:
                st.markdown("---")
                st.subheader("üìä Detailed Compliance Results")
                
                # Create tabs for different views
                tab1, tab2, tab3 = st.tabs(["üìã Full Checklist", "‚ùå Failed Items", "üìà Analysis"])
                
                with tab1:
                    # Complete checklist with expandable sections
                    for section_name, subsections in DD_CHECKLIST.items():
                        with st.expander(f"üìë {section_name}", expanded=True):
                            
                            section_stats = {'passed': 0, 'failed': 0, 'pending': 0}
                            
                            for subsection_name, items in subsections.items():
                                st.markdown(f"**{subsection_name}**")
                                
                                for item in items:
                                    status = st.session_state.checklist_status.get(item, (None, "Not checked"))
                                    
                                    if status[0] is True:
                                        st.success(f"‚úÖ {item}")
                                        st.caption(f"üí° {status[1]}")
                                        section_stats['passed'] += 1
                                    elif status[0] is False:
                                        st.error(f"‚ùå {item}")
                                        st.caption(f"üö® Issue: {status[1]}")
                                        section_stats['failed'] += 1
                                    else:
                                        st.warning(f"‚è≥ {item}")
                                        st.caption(f"üìù {status[1]}")
                                        section_stats['pending'] += 1
                                
                                st.markdown("---")
                            
                            # Section summary
                            total_section = sum(section_stats.values())
                            if total_section > 0:
                                section_score = section_stats['passed'] / total_section * 100
                                st.markdown(f"**{section_name} Compliance: {section_stats['passed']}/{total_section} ({section_score:.1f}%)**")
                
                with tab2:
                    # Failed items focus
                    failed_items = [(item, status) for item, status in st.session_state.checklist_status.items() if status[0] is False]
                    
                    if failed_items:
                        st.error(f"‚ùå **{len(failed_items)} items require immediate attention:**")
                        
                        for i, (item, status) in enumerate(failed_items, 1):
                            st.markdown(f"**{i}. {item}**")
                            st.write(f"   üö® **Issue:** {status[1]}")
                            st.write(f"   üí° **Action:** Review and remediate before closing")
                            st.markdown("---")
                    else:
                        st.success("üéâ No failed compliance items!")
                
                with tab3:
                    # Statistical analysis
                    total_items = len(st.session_state.checklist_status)
                    passed_items = sum(1 for status in st.session_state.checklist_status.values() if status[0] is True)
                    failed_items = sum(1 for status in st.session_state.checklist_status.values() if status[0] is False)
                    pending_items = total_items - passed_items - failed_items
                    
                    # Metrics display
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Total Checks", total_items)
                    with col2:
                        st.metric("‚úÖ Passed", passed_items)
                    with col3:
                        st.metric("‚ùå Failed", failed_items)
                    with col4:
                        st.metric("‚è≥ Pending", pending_items)
                    
                    # Compliance trends
                    compliance_data = {
                        'Status': ['Passed', 'Failed', 'Pending'],
                        'Count': [passed_items, failed_items, pending_items],
                        'Percentage': [
                            passed_items/total_items*100 if total_items > 0 else 0,
                            failed_items/total_items*100 if total_items > 0 else 0,
                            pending_items/total_items*100 if total_items > 0 else 0
                        ]
                    }
                    
                    fig1 = px.bar(
                        x=compliance_data['Status'],
                        y=compliance_data['Count'],
                        title="Compliance Check Results",
                        color=compliance_data['Status'],
                        color_discrete_map={'Passed': 'green', 'Failed': 'red', 'Pending': 'orange'}
                    )
                    st.plotly_chart(fig1, use_container_width=True)
                    
                    # Section-by-section breakdown
                    section_data = []
                    for section_name, subsections in DD_CHECKLIST.items():
                        section_items = [item for subsection in subsections.values() for item in subsection]
                        section_passed = sum(1 for item in section_items 
                                           if st.session_state.checklist_status.get(item, (None, ""))[0] is True)
                        section_total = len(section_items)
                        section_score = section_passed / section_total * 100 if section_total > 0 else 0
                        
                        section_data.append({
                            'Section': section_name,
                            'Score': section_score,
                            'Passed': section_passed,
                            'Total': section_total
                        })
                    
                    section_df = pd.DataFrame(section_data)
                    st.dataframe(section_df, use_container_width=True)
        
        with col2:
            st.subheader("üìä Compliance Dashboard")
            
            if st.session_state.checklist_status:
                # Real-time compliance metrics
                total_items = len(st.session_state.checklist_status)
                passed_items = sum(1 for status in st.session_state.checklist_status.values() if status[0] is True)
                failed_items = sum(1 for status in st.session_state.checklist_status.values() if status[0] is False)
                pending_items = total_items - passed_items - failed_items
                
                # Compliance score
                compliance_score = (passed_items / total_items * 100) if total_items > 0 else 0
                
                # Enhanced metrics
                st.metric("üìã Total Items", total_items)
                st.metric("‚úÖ Passed", f"{passed_items} ({passed_items/total_items*100:.1f}%)")
                st.metric("‚ùå Failed", f"{failed_items} ({failed_items/total_items*100:.1f}%)")
                st.metric("‚è≥ Pending", f"{pending_items} ({pending_items/total_items*100:.1f}%)")
                
                # Overall compliance score with color coding
                if compliance_score >= 90:
                    st.success(f"üéØ **Compliance Score: {compliance_score:.1f}%**\n\n‚úÖ Excellent compliance!")
                elif compliance_score >= 75:
                    st.warning(f"üéØ **Compliance Score: {compliance_score:.1f}%**\n\n‚ö†Ô∏è Good with some issues")
                else:
                    st.error(f"üéØ **Compliance Score: {compliance_score:.1f}%**\n\nüö® Requires attention")
                
                # Visual compliance chart
                if total_items > 0:
                    fig = px.pie(
                        values=[passed_items, failed_items, pending_items],
                        names=['Passed ‚úÖ', 'Failed ‚ùå', 'Pending ‚è≥'],
                        title="Compliance Status Overview",
                        color_discrete_map={
                            'Passed ‚úÖ': '#28a745',
                            'Failed ‚ùå': '#dc3545', 
                            'Pending ‚è≥': '#ffc107'
                        }
                    )
                    fig.update_traces(textposition='inside', textinfo='percent+label')
                    st.plotly_chart(fig, use_container_width=True)
                
                # Risk assessment
                st.subheader("‚öñÔ∏è Risk Assessment")
                if failed_items == 0:
                    st.success("üü¢ **LOW RISK**\n\nReady to proceed")
                elif failed_items <= 3:
                    st.warning("üü° **MEDIUM RISK**\n\nAddress issues first")
                else:
                    st.error("üî¥ **HIGH RISK**\n\nSignificant remediation needed")
            
            else:
                st.info("üìä Run compliance check to see results")

elif workflow_step == "üìä Tie-Out Report":
    st.header("üìä Professional Cap Table Tie-Out Report")
    
    if not st.session_state.checklist_status:
        st.warning("‚ö†Ô∏è Please complete compliance check first.")
        st.info("üëà Use the sidebar to navigate back to Compliance Check")
    else:
        st.info("üìã **Final comprehensive report** with all findings, issues, and recommendations")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üìã Executive Summary & Complete Report")
            
            # Generate comprehensive report
            report_content = generate_tie_out_report(
                st.session_state.analysis_results,
                st.session_state.checklist_status,
                st.session_state.discrepancies
            )
            
            # Display report with enhanced formatting
            st.markdown(report_content)
            
            # Enhanced download options
            st.markdown("---")
            st.subheader("üì• Download Options")
            
            col1_dl, col2_dl, col3_dl = st.columns(3)
            
            with col1_dl:
                # Markdown report
                st.download_button(
                    label="üìÑ Download Report (Markdown)",
                    data=report_content,
                    file_name=f"cap_table_tieout_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown",
                    help="Download as Markdown file for easy editing"
                )
            
            with col2_dl:
                # Plain text version
                plain_text = report_content.replace('#', '').replace('**', '').replace('*', '')
                st.download_button(
                    label="üìù Download Report (Text)",
                    data=plain_text,
                    file_name=f"cap_table_tieout_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain",
                    help="Download as plain text file"
                )
            
            with col3_dl:
                # JSON data export
                export_data = {
                    'analysis_results': st.session_state.analysis_results,
                    'compliance_status': st.session_state.checklist_status,
                    'discrepancies': st.session_state.discrepancies,
                    'generated_at': datetime.now().isoformat()
                }
                
                st.download_button(
                    label="üíæ Download Data (JSON)",
                    data=json.dumps(export_data, indent=2, default=str),
                    file_name=f"cap_table_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    help="Download raw analysis data for further processing"
                )
        
        with col2:
            st.subheader("üéØ Final Assessment")
            
            # Comprehensive final metrics
            if st.session_state.analysis_results and st.session_state.checklist_status:
                total_docs = len(st.session_state.analysis_results)
                total_compliance_items = len(st.session_state.checklist_status)
                total_discrepancies = len(st.session_state.discrepancies)
                
                passed_items = sum(1 for status in st.session_state.checklist_status.values() if status[0] is True)
                compliance_score = (passed_items / total_compliance_items * 100) if total_compliance_items > 0 else 0
                
                # Key metrics
                st.metric("üìÑ Documents Analyzed", total_docs)
                st.metric("‚úÖ Compliance Items Checked", total_compliance_items)
                st.metric("üéØ Compliance Score", f"{compliance_score:.1f}%")
                st.metric("üö® Issues Found", total_discrepancies)
                
                # Overall risk assessment with detailed explanation
                st.subheader("‚öñÔ∏è Risk Assessment")
                if total_discrepancies == 0 and compliance_score >= 95:
                    st.success("üü¢ **EXCELLENT STANDING**")
                    st.write("‚úÖ Ready for transaction closing")
                    st.write("‚úÖ No significant issues identified")
                    st.write("‚úÖ Strong compliance posture")
                elif total_discrepancies <= 2 and compliance_score >= 85:
                    st.success("üü¢ **LOW RISK**")
                    st.write("‚úÖ Generally ready to proceed")
                    st.write("‚ö†Ô∏è Minor issues to address")
                    st.write("üìã Standard due diligence process")
                elif total_discrepancies <= 5 and compliance_score >= 70:
                    st.warning("üü° **MEDIUM RISK**")
                    st.write("‚ö†Ô∏è Several issues require attention")
                    st.write("üîß Remediation needed before closing")
                    st.write("üë®‚Äçüíº Legal counsel recommended")
                else:
                    st.error("üî¥ **HIGH RISK**")
                    st.write("üö® Significant issues identified")
                    st.write("üõë Transaction may be delayed")
                    st.write("‚öñÔ∏è Immediate legal review required")
                
                # Action items summary
                st.subheader("üìã Immediate Action Items")
                
                if total_discrepancies > 0:
                    priority_actions = [
                        f"Address {len([d for d in st.session_state.discrepancies if d.get('severity') == 'High'])} high-priority issues",
                        "Obtain missing board approvals and documentation",
                        "Correct cap table discrepancies",
                        "Update corporate records",
                        "Prepare remediation timeline"
                    ]
                    
                    for i, action in enumerate(priority_actions, 1):
                        if i <= total_discrepancies:
                            st.write(f"{i}. {action}")
                else:
                    st.write("1. ‚úÖ Review and confirm all extracted data")
                    st.write("2. ‚úÖ Finalize transaction documentation")
                    st.write("3. ‚úÖ Proceed with closing preparation")
                
                # Timeline estimate
                st.subheader("‚è∞ Estimated Timeline")
                if total_discrepancies == 0:
                    st.success("**Ready to close:** 1-2 weeks")
                elif total_discrepancies <= 3:
                    st.warning("**Remediation needed:** 2-4 weeks")
                else:
                    st.error("**Significant work required:** 4-8 weeks")
            
            # Sample cap table display
            st.subheader("üìä Reference Cap Table")
            if st.session_state.current_cap_table is None:
                st.session_state.current_cap_table = create_sample_cap_table()
            
            # Enhanced cap table with analysis integration
            cap_table = st.session_state.current_cap_table.copy()
            
            # Add analysis insights if available
            if st.session_state.extracted_cap_data:
                st.info("üí° Cap table data has been cross-referenced with uploaded documents")
            
            st.dataframe(cap_table, use_container_width=True)
            
            # Cap table insights
            total_outstanding = cap_table['Shares_Outstanding'].sum()
            total_ownership = cap_table['Ownership_Percentage'].sum()
            
            st.write(f"**Total Outstanding:** {total_outstanding:,} shares")
            st.write(f"**Total Ownership:** {total_ownership:.1f}%")
            
            if abs(total_ownership - 100) > 0.1:
                st.warning("‚ö†Ô∏è Ownership percentages don't sum to 100%")

# Enhanced Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px; margin: 20px 0;'>
    <h3>‚öñÔ∏è Cap Table Tie-Out Analysis Tool</h3>
    <p><strong>Professional AI-Powered Legal Document Analysis & Compliance Verification</strong></p>
    <p>ü§ñ Powered by Claude AI | üèóÔ∏è Built with Streamlit | ‚ö° Enterprise-Ready</p>
    <p><em>This tool provides automated analysis assistance for due diligence processes.<br/>
    Always consult qualified legal counsel for final review and verification.</em></p>
</div>
""", unsafe_allow_html=True)

# Additional helpful information
with st.expander("‚ÑπÔ∏è **Important Disclaimers & Legal Notes**"):
    st.markdown("""
    ### üìã Usage Guidelines
    
    **‚úÖ This tool is designed to:**
    - Streamline due diligence document review
    - Identify potential compliance issues
    - Extract structured data from legal documents
    - Generate comprehensive analysis reports
    - Support legal professionals in cap table verification
    
    **‚ö†Ô∏è Important Limitations:**
    - AI analysis should supplement, not replace, legal review
    - Complex legal provisions may require human interpretation
    - OCR errors in scanned documents may affect accuracy
    - Cross-references are limited to uploaded documents only
    - Securities law compliance requires qualified legal counsel
    
    **üîí Security & Privacy:**
    - Documents are processed via Anthropic's Claude API
    - No documents are permanently stored by this application
    - API calls are subject to Anthropic's privacy policy
    - Users responsible for handling confidential information appropriately
    
    **‚öñÔ∏è Legal Disclaimer:**
    This software is provided for informational purposes only and does not constitute legal advice. 
    Users should consult with qualified attorneys familiar with securities law and corporate governance 
    for definitive legal guidance. The creators assume no liability for decisions made based on this analysis.
    
    **üìû Support:**
    For technical issues or questions about this tool, please consult your legal team or 
    the documentation provided with your implementation.
    """)

# Performance monitoring (optional - for production use)
if st.sidebar.button("üîß System Status", help="Check system performance"):
    with st.sidebar:
        st.write("**System Status:**")
        st.write(f"‚úÖ Claude API: {'Connected' if get_anthropic_client() else 'Disconnected'}")
        st.write(f"üìÑ Documents: {len(st.session_state.uploaded_files)}")
        st.write(f"ü§ñ Analyses: {len(st.session_state.analysis_results)}")
        st.write(f"‚úÖ Compliance: {len(st.session_state.checklist_status)}")
        st.write(f"‚ö†Ô∏è Issues: {len(st.session_state.discrepancies)}")
        
        # Memory usage estimate
        import sys
        total_size = sum(sys.getsizeof(v) for v in st.session_state.values())
        st.write(f"üíæ Memory: ~{total_size/1024:.1f} KB")

# Debug mode (optional - can be removed for production)
if st.sidebar.checkbox("üêõ Debug Mode", help="Show technical details"):
    with st.expander("üîß Technical Debug Information"):
        st.write("**Session State Variables:**")
        for key, value in st.session_state.items():
            if isinstance(value, dict):
                st.write(f"- **{key}:** {len(value)} items")
            elif isinstance(value, list):
                st.write(f"- **{key}:** {len(value)} items")
            else:
                st.write(f"- **{key}:** {type(value).__name__}")
        
        st.write("**Environment:**")
        st.write(f"- Claude API Key: {'‚úÖ Configured' if get_anthropic_client() else '‚ùå Missing'}")
        st.write(f"- Streamlit Version: {st.__version__}")
        
        if st.button("Clear All Data"):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.success("‚úÖ All session data cleared!")
            st.rerun()import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import json
import io
import base64
from typing import Dict, List, Any
import re
import numpy as np
import anthropic
import PyPDF2
from docx import Document
import openpyxl
import os

# Configure page
st.set_page_config(
    page_title="Cap Table Tie-Out Analysis Tool",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = {}
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = {}
if 'checklist_status' not in st.session_state:
    st.session_state.checklist_status = {}
if 'current_cap_table' not in st.session_state:
    st.session_state.current_cap_table = None
if 'discrepancies' not in st.session_state:
    st.session_state.discrepancies = []
if 'extracted_cap_data' not in st.session_state:
    st.session_state.extracted_cap_data = {}

# Due Diligence Checklist Structure - Complete Implementation
DD_CHECKLIST = {
    "Capitalization Audit": {
        "Charter & Authorization": [
            "Charter properly approved and filed with state",
            "Sufficient shares authorized to cover all issuances",
            "Share classes properly defined in charter",
            "Board size and voting requirements documented",
            "Stockholder voting requirements documented"
        ],
        "Common Stock Verification": [
            "Stock issuance approved by Board and reflected on cap table",
            "Stock purchase agreements exist and match cap table shares/names",
            "Issuance complies with governance documents and preemptive rights",
            "Transfers properly documented and compliant with ROFR restrictions",
            "Founder vesting schedules documented with acceleration terms"
        ],
        "Preferred Stock Verification": [
            "Preferred stock issuance Board-approved and charter-compliant",
            "Purchase agreements match cap table in shares/price/name",
            "Liquidation preferences and conversion terms accurate",
            "Dividend and voting rights properly documented",
            "Anti-dilution and preemptive rights compliance verified"
        ],
        "Option Pool Verification": [
            "Board approved option grants with proper strike prices",
            "409A valuation in place within 1-year safe harbor",
            "Grant details in Board approval match cap table exactly",
            "Stock option agreements match cap table terms",
            "Vesting schedules and acceleration terms documented",
            "Plan addresses option treatment on change of control"
        ],
        "Warrant Verification": [
            "Warrant issuance Board-approved and reflected on cap table",
            "Warrant agreements match cap table in number/type/holder name",
            "Exercise terms and expiration dates properly documented",
            "Termination and cash-out provisions for sale scenarios",
            "Anti-dilution and special rights documented"
        ],
        "Convertible Instruments": [
            "SAFE/Note issuance Board-approved and cap table reflected",
            "Conversion terms match pro forma calculations",
            "Valuation caps and discount rates accurate",
            "Automatic conversion triggers properly defined",
            "MFN and side letter provisions documented"
        ],
        "Price & Valuation Verification": [
            "Preferred stock price maps to pre-money valuation correctly",
            "Post-money valuation calculations verified",
            "Client ownership percentage equals investment √∑ post-money",
            "409A valuations current and support option strike prices",
            "Liquidation waterfall calculations verified"
        ]
    }
}

# Initialize Anthropic client
@st.cache_resource
def get_anthropic_client():
    try:
        api_key = os.getenv("ANTHROPIC_API_KEY") or st.secrets.get("ANTHROPIC_API_KEY")
        if not api_key:
            return None
        return anthropic.Anthropic(api_key=api_key)
    except Exception as e:
        st.error(f"Error initializing Claude API: {str(e)}")
        return None

def extract_text_from_pdf(file_content):
    """Extract text from PDF file with error handling"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
        text = ""
        for page_num, page in enumerate(pdf_reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
            except Exception as e:
                st.warning(f"Could not extract text from page {page_num + 1}: {str(e)}")
        return text if text else "Could not extract text from PDF"
    except Exception as e:
        st.error(f"Error reading PDF: {str(e)}")
        return f"PDF processing error: {str(e)}"

def extract_text_from_docx(file_content):
    """Extract text from DOCX file with error handling"""
    try:
        doc = Document(io.BytesIO(file_content))
        text = ""
        for para_num, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():
                text += f"{paragraph.text}\n"
        
        # Also extract table content
        for table in doc.tables:
            text += "\n--- Table ---\n"
            for row in table.rows:
                row_text = " | ".join([cell.text.strip() for cell in row.cells])
                if row_text.strip():
                    text += f"{row_text}\n"
        
        return text if text else "No text content found in document"
    except Exception as e:
        st.error(f"Error reading DOCX: {str(e)}")
        return f"DOCX processing error: {str(e)}"

def extract_text_from_xlsx(file_content):
    """Extract structured data from Excel file"""
    try:
        workbook = openpyxl.load_workbook(io.BytesIO(file_content), data_only=True)
        text = ""
        
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]
            text += f"\n=== Sheet: {sheet_name} ===\n"
            
            # Get headers from first row
            headers = []
            first_row = next(sheet.iter_rows(min_row=1, max_row=1, values_only=True), None)
            if first_row:
                headers = [str(cell) if cell is not None else f"Col_{i}" for i, cell in enumerate(first_row)]
                text += f"Headers: {' | '.join(headers)}\n"
            
            # Extract data rows
            for row_num, row in enumerate(sheet.iter_rows(min_row=2, values_only=True), start=2):
                if any(cell is not None for cell in row):
                    row_data = []
                    for i, cell in enumerate(row):
                        header = headers[i] if i < len(headers) else f"Col_{i}"
                        value = str(cell) if cell is not None else ""
                        if value:
                            row_data.append(f"{header}: {value}")
                    
                    if row_data:
                        text += f"Row {row_num}: {' | '.join(row_data)}\n"
            
            text += "\n"
        
        return text if text else "No data found in Excel file"
    except Exception as e:
        st.error(f"Error reading Excel: {str(e)}")
        return f"Excel processing error: {str(e)}"

def process_uploaded_file(uploaded_file):
    """Process uploaded file and extract text content with comprehensive error handling"""
    try:
        file_content = uploaded_file.read()
        
        if uploaded_file.type == "application/pdf":
            return extract_text_from_pdf(file_content)
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            return extract_text_from_docx(file_content)
        elif uploaded_file.type in [
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", 
            "application/vnd.ms-excel"
        ]:
            return extract_text_from_xlsx(file_content)
        elif uploaded_file.type in ["text/plain", "text/csv"]:
            try:
                return file_content.decode('utf-8')
            except UnicodeDecodeError:
                return file_content.decode('utf-8', errors='ignore')
        else:
            # Try to decode as text
            try:
                return file_content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    return file_content.decode('latin-1')
                except:
                    return f"Unsupported file type: {uploaded_file.type}"
    
    except Exception as e:
        st.error(f"Error processing file {uploaded_file.name}: {str(e)}")
        return f"File processing error: {str(e)}"

def analyze_document_with_ai(file_content, file_name, document_type):
    """Analyze document content using Claude API with comprehensive prompts"""
    client = get_anthropic_client()
    
    if not client:
        st.error("‚ùå Claude API key not configured. Please check your API key setup.")
        return {
            'document_type': document_type,
            'file_name': file_name,
            'extracted_data': {},
            'compliance_items': [],
            'issues_found': ["API key not configured - cannot perform analysis"],
            'confidence_score': 0.0,
            'raw_response': "API unavailable"
        }
    
    # Limit content to avoid token limits while preserving important information
    content_preview = file_content[:6000] if len(file_content) > 6000 else file_content
    
    # Create comprehensive analysis prompts
    if 'charter' in document_type.lower() or 'incorporation' in document_type.lower():
        prompt = f"""
        You are a legal expert analyzing a Certificate of Incorporation for cap table tie-out purposes.
        
        Document: {file_name}
        Content: {content_preview}
        
        Extract the following information and return ONLY a valid JSON object:
        {{
            "authorized_shares": {{"common": <number>, "preferred": <number>}},
            "par_value": <number>,
            "incorporation_date": "YYYY-MM-DD",
            "state_of_incorporation": "state",
            "share_classes": ["list of share classes"],
            "board_size": <number or "variable">,
            "special_provisions": ["list"],
            "compliance_items": [
                "Charter filed with Secretary of State",
                "Share classes properly defined",
                "Board composition requirements specified"
            ],
            "issues_found": ["list any missing or problematic items"]
        }}
        
        Focus on: authorized share counts, par values, share class definitions, and any governance provisions.
        """
    
    elif 'stock purchase' in document_type.lower() or 'spa' in document_type.lower():
        prompt = f"""
        You are a legal expert analyzing a Stock Purchase Agreement for cap table tie-out.
        
        Document: {file_name}
        Content: {content_preview}
        
        Extract and return ONLY a valid JSON object:
        {{
            "purchaser_name": "investor name",
            "shares_purchased": <number>,
            "price_per_share": <number>,
            "total_consideration": <number>,
            "purchase_date": "YYYY-MM-DD",
            "share_class": "class type",
            "closing_conditions": ["list"],
            "board_approval_reference": "reference or date",
            "preemptive_rights_waiver": true/false,
            "compliance_items": [
                "Board approval documented",
                "Purchase price calculated correctly",
                "Closing conditions satisfied"
            ],
            "issues_found": ["list any compliance concerns"]
        }}
        
        Focus on: share quantities, pricing, board approvals, and closing mechanics.
        """
    
    elif 'option' in document_type.lower():
        prompt = f"""
        You are analyzing a Stock Option Grant for cap table compliance.
        
        Document: {file_name}
        Content: {content_preview}
        
        Extract and return ONLY a valid JSON object:
        {{
            "grantee_name": "employee name",
            "shares_granted": <number>,
            "exercise_price": <number>,
            "grant_date": "YYYY-MM-DD",
            "vesting_schedule": "description",
            "vesting_cliff": "period",
            "expiration_date": "YYYY-MM-DD",
            "board_approval_date": "YYYY-MM-DD",
            "valuation_409a_date": "YYYY-MM-DD",
            "acceleration_provisions": ["list"],
            "compliance_items": [
                "Board approval documented",
                "409A valuation current",
                "Exercise price equals FMV",
                "Vesting schedule compliant"
            ],
            "issues_found": ["list compliance issues"]
        }}
        
        Focus on: exercise prices, 409A compliance, board approvals, and vesting terms.
        """
    
    elif 'warrant' in document_type.lower():
        prompt = f"""
        You are analyzing a Warrant Agreement for cap table tie-out.
        
        Document: {file_name}
        Content: {content_preview}
        
        Extract and return ONLY a valid JSON object:
        {{
            "warrant_holder": "holder name",
            "shares_underlying": <number>,
            "exercise_price": <number>,
            "issue_date": "YYYY-MM-DD",
            "expiration_date": "YYYY-MM-DD",
            "exercise_period": "description",
            "anti_dilution_provisions": ["list"],
            "termination_provisions": ["list"],
            "cashless_exercise": true/false,
            "compliance_items": [
                "Board approval documented",
                "Exercise terms clearly defined",
                "Expiration date specified",
                "Termination provisions included"
            ],
            "issues_found": ["list any issues"]
        }}
        
        Focus on: exercise mechanics, termination rights, and anti-dilution protections.
        """
    
    elif 'safe' in document_type.lower() or 'convertible' in document_type.lower():
        prompt = f"""
        You are analyzing a SAFE or Convertible Note for cap table analysis.
        
        Document: {file_name}
        Content: {content_preview}
        
        Extract and return ONLY a valid JSON object:
        {{
            "investor_name": "investor",
            "investment_amount": <number>,
            "valuation_cap": <number>,
            "discount_rate": <decimal>,
            "issue_date": "YYYY-MM-DD",
            "conversion_triggers": ["list"],
            "mfn_provision": true/false,
            "pro_rata_rights": true/false,
            "information_rights": true/false,
            "side_letter_provisions": ["list"],
            "compliance_items": [
                "Investment amount confirmed",
                "Conversion mechanics defined",
                "Valuation cap specified",
                "Discount rate documented"
            ],
            "issues_found": ["list issues"]
        }}
        
        Focus on: conversion mechanics, valuation caps, discounts, and investor rights.
        """
    
    elif 'cap' in document_type.lower() or 'table' in document_type.lower():
        prompt = f"""
        You are analyzing a Cap Table for completeness and accuracy.
        
        Document: {file_name}
        Content: {content_preview}
        
        Extract and return ONLY a valid JSON object:
        {{
            "total_common_outstanding": <number>,
            "total_preferred_outstanding": <number>,
            "option_pool_size": <number>,
            "options_granted": <number>,
            "options_available": <number>,
            "fully_diluted_shares": <number>,
            "major_shareholders": [
                {{"name": "name", "shares": <number>, "percentage": <decimal>, "class": "class"}}
            ],
            "share_classes": ["list of classes"],
            "last_update_date": "YYYY-MM-DD",
            "compliance_items": [
                "All share classes accounted for",
                "Option pool properly allocated",
                "Ownership percentages calculated",
                "Recent update confirmed"
            ],
            "issues_found": ["list discrepancies or missing data"]
        }}
        
        Focus on: share counts, ownership percentages, option pools, and data completeness.
        """
    
    else:
        prompt = f"""
        Analyze this legal document for any capitalization-related information.
        
        Document: {file_name}
        Content: {content_preview}
        
        Look for: shares, equity, ownership, options, warrants, convertible instruments, valuations, board approvals.
        
        Return ONLY a valid JSON object:
        {{
            "document_summary": "brief description",
            "capitalization_mentions": ["list relevant mentions"],
            "key_terms": ["important terms found"],
            "dates_mentioned": ["YYYY-MM-DD"],
            "compliance_items": ["relevant compliance points"],
            "issues_found": ["potential concerns"]
        }}
        """
    
    try:
        # Call Claude API with comprehensive error handling
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=3000,
            temperature=0.1,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        # Parse response
        response_text = response.content[0].text.strip()
        
        # Extract JSON from response
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        
        if json_start >= 0 and json_end > json_start:
            json_text = response_text[json_start:json_end]
            try:
                extracted_data = json.loads(json_text)
            except json.JSONDecodeError as e:
                st.warning(f"JSON parsing error for {file_name}: {str(e)}")
                extracted_data = {
                    "parsing_error": str(e),
                    "raw_response": response_text[:500]
                }
        else:
            extracted_data = {"raw_response": response_text}
        
        # Store extracted data for cap table cross-referencing
        st.session_state.extracted_cap_data[file_name] = extracted_data
        
        return {
            'document_type': document_type,
            'file_name': file_name,
            'extracted_data': extracted_data,
            'compliance_items': extracted_data.get('compliance_items', []),
            'issues_found': extracted_data.get('issues_found', []),
            'confidence_score': 0.9,  # High confidence for actual API results
            'raw_response': response_text
        }
        
    except Exception as e:
        error_msg = f"Claude API error: {str(e)}"
        st.error(error_msg)
        return {
            'document_type': document_type,
            'file_name': file_name,
            'extracted_data': {"api_error": error_msg},
            'compliance_items': [],
            'issues_found': [error_msg],
            'confidence_score': 0.0
        }

def create_sample_cap_table():
    """Create a realistic sample cap table"""
    return pd.DataFrame({
        'Security_Type': [
            'Common Stock', 'Common Stock', 'Series A Preferred', 'Series A Preferred', 
            'Series B Preferred', 'Employee Options (Granted)', 'Employee Options (Available)', 
            'Warrant', 'SAFE Note (Unconverted)'
        ],
        'Holder_Name': [
            'Founder A', 'Founder B', 'Acme Ventures', 'Strategic Investor', 
            'Growth Capital Partners', 'Employee Pool (Granted)', 'Employee Pool (Available)', 
            'Advisor 1', 'Angel Investment LLC'
        ],
        'Shares_Outstanding': [3000000, 2000000, 1500000, 500000, 1000000, 200000, 300000, 15000, 0],
        'Shares_Authorized': [10000000, 10000000, 2000000, 2000000, 1500000, 1000000, 1000000, 50000, 0],
        'Price_Per_Share': [0.001, 0.001, 1.50, 1.50, 3.00, 0.75, 0.75, 1.50, 0],
        'Ownership_Percentage': [35.7, 23.8, 17.9, 6.0, 11.9, 2.4, 0, 0.2, 2.1],
        'Liquidation_Preference': ['None', 'None', '1x Non-Participating', '1x Non-Participating', 
                                 '1x Participating', 'None', 'None', 'None', 'Converts'],
        'Vesting_Schedule': ['4yr/1yr cliff', '4yr/1yr cliff', 'None', 'None', 'None', 
                           '4yr/1yr cliff', '4yr/1yr cliff', '3yr/6mo cliff', 'None'],
        'Issue_Date': ['2021-01-01', '2021-01-01', '2022-03-15', '2022-03-15', '2023-09-01', 
                      '2021-06-01', '2023-01-01', '2021-12-01', '2023-01-15']
    })

def check_compliance_item(item, analysis_results):
    """Perform actual compliance checking based on extracted data"""
    
    # 409A Valuation Checks
    if "409A valuation" in item:
        current_409a_dates = []
        for doc_analysis in analysis_results.values():
            extracted = doc_analysis.get('extracted_data', {})
            if 'valuation_409a_date' in extracted:
                try:
                    val_date = datetime.strptime(extracted['valuation_409a_date'], '%Y-%m-%d')
                    current_409a_dates.append(val_date)
                except:
                    pass
        
        if current_409a_dates:
            latest_409a = max(current_409a_dates)
            days_since = (datetime.now() - latest_409a).days
            if days_since <= 365:
                return True, f"409A valuation current (dated {latest_409a.strftime('%Y-%m-%d')}, {days_since} days ago)"
            else:
                return False, f"409A valuation stale (dated {latest_409a.strftime('%Y-%m-%d')}, {days_since} days ago)"
        else:
            return False, "No 409A valuation dates found in documents"
    
    # Board Approval Checks
    elif "Board approval" in item:
        board_approvals = []
        for doc_analysis in analysis_results.values():
            extracted = doc_analysis.get('extracted_data', {})
            if 'board_approval_date' in extracted or 'board_approval_reference' in extracted:
                board_approvals.append(doc_analysis['file_name'])
            
            # Check compliance items for board approval mentions
            compliance_items = doc_analysis.get('compliance_items', [])
            if any('board approval' in item.lower() for item in compliance_items):
                board_approvals.append(doc_analysis['file_name'])
        
        if board_approvals:
            return True, f"Board approval found in: {', '.join(set(board_approvals))}"
        else:
            return False, "No board approval documentation found"
    
    # Cap Table Matching Checks
    elif "matches cap table" in item or "match cap table" in item:
        cap_table_docs = [name for name in analysis_results.keys() if 'cap' in name.lower()]
        if not cap_table_docs:
            return None, "No cap table document uploaded for comparison"
        
        # Perform basic data consistency check
        share_counts = {}
        for doc_analysis in analysis_results.values():
            extracted = doc_analysis.get('extracted_data', {})
            if 'shares_purchased' in extracted:
                share_counts[doc_analysis['file_name']] = extracted['shares_purchased']
            if 'shares_granted' in extracted:
                share_counts[doc_analysis['file_name']] = extracted['shares_granted']
        
        if len(share_counts) > 1:
            return True, f"Share data found in {len(share_counts)} documents for cross-reference"
        else:
            return None, "Insufficient data for cap table cross-reference"
    
    # Share Authorization Checks
    elif "authorized" in item and "shares" in item:
        authorized_shares = {}
        for doc_analysis in analysis_results.values():
            extracted = doc_analysis.get('extracted_data', {})
            if 'authorized_shares' in extracted:
                authorized_shares[doc_analysis['file_name']] = extracted['authorized_shares']
        
        if authorized_shares:
            return True, f"Authorized shares documented in: {', '.join(authorized_shares.keys())}"
        else:
            return False, "No authorized share information found"
    
    # Price and Valuation Checks
    elif "price" in item and ("valuation" in item or "maps to" in item):
        pricing_data = {}
        for doc_analysis in analysis_results.values():
            extracted = doc_analysis.get('extracted_data', {})
            if 'price_per_share' in extracted and 'total_consideration' in extracted:
                pricing_data[doc_analysis['file_name']] = {
                    'price': extracted['price_per_share'],
                    'total': extracted['total_consideration']
                }
        
        if pricing_data:
            return True, f"Pricing data found in: {', '.join(pricing_data.keys())}"
        else:
            return False, "No pricing/valuation data found for verification"
    
    # Charter and Filing Checks
    elif "charter" in item and ("filed" in item or "approved" in item):
        charter_docs = [name for name, analysis in analysis_results.items() 
                       if 'charter' in analysis['document_type'].lower() or 'incorporation' in analysis['document_type'].lower()]
        
        if charter_docs:
            return True, f"Charter documentation found: {', '.join(charter_docs)}"
        else:
            return False, "No charter/incorporation documents uploaded"
    
    # Conversion Terms Checks
    elif "conversion" in item:
        conversion_docs = []
        for doc_analysis in analysis_results.values():
            extracted = doc_analysis.get('extracted_data', {})
            if 'conversion_triggers' in extracted or 'conversion_terms' in extracted:
                conversion_docs.append(doc_analysis['file_name'])
        
        if conversion_docs:
            return True, f"Conversion terms found in: {', '.join(conversion_docs)}"
        else:
            return False, "No conversion terms documentation found"
    
    # Vesting Schedule Checks
    elif "vesting" in item:
        vesting_docs = []
        for doc_analysis in analysis_results.values():
            extracted = doc_analysis.get('extracted_data', {})
            if 'vesting_schedule' in extracted or 'acceleration_provisions' in extracted:
                vesting_docs.append(doc_analysis['file_name'])
        
        if vesting_docs:
            return True, f"Vesting documentation found in: {', '.join(vesting_docs)}"
        else:
            return False, "No vesting schedule documentation found"
    
    # Default case - requires manual review
    else:
        return None, "Requires manual review - automated check not available"

def generate_tie_out_report(analysis_results, checklist_status, discrepancies):
    """Generate a comprehensive, professional tie-out report"""
    
    # Calculate summary statistics
    total_docs = len(analysis_results)
    total_items = sum(len(items) for section in DD_CHECKLIST.values() for items in section.values())
    passed_items = sum(1 for status in checklist_status.values() if status[0] is True)
    failed_items = sum(1 for status in checklist_status.values() if status[0] is False)
    pending_items = total_items - passed_items - failed_items
    
    compliance_score = (passed_items / total_items * 100) if total_items > 0 else 0
    avg_confidence = np.mean([doc.get('confidence_score', 0) for doc in analysis_results.values()]) * 100
    
    report = f"""# Cap Table Tie-Out Analysis Report

**Company:** [Company Name]  
**Analysis Date:** {datetime.now().strftime('%B %d, %Y')}  
**Prepared By:** Cap Table Tie-Out AI Tool  
**Report ID:** CTTO-{datetime.now().strftime('%Y%m%d-%H%M%S')}

---

## Executive Summary

This report presents the results of an automated cap table tie-out analysis performed on **{total_docs} legal documents** using AI-powered document analysis and compliance verification.

### Key Findings Summary:
- **üìä Documents Analyzed:** {total_docs}
- **‚úÖ Compliance Items Passed:** {passed_items}/{total_items} ({compliance_score:.1f}%)
- **‚ùå Issues Identified:** {len(discrepancies)}
- **ü§ñ AI Confidence Score:** {avg_confidence:.1f}%
- **‚öñÔ∏è Overall Risk Level:** {"üü¢ LOW" if len(discrepancies) <= 2 else "üü° MEDIUM" if len(discrepancies) <= 5 else "üî¥ HIGH"}

### Document Analysis Summary:
"""
    
    for filename, analysis in analysis_results.items():
        doc_type = analysis['document_type']
        confidence = analysis['confidence_score'] * 100
        issues = len(analysis['issues_found'])
        report += f"- **{filename}** ({doc_type}) - Confidence: {confidence:.1f}%, Issues: {issues}\n"
    
    report += f"""

---

## Detailed Compliance Analysis

The following analysis covers all required due diligence items for cap table tie-out:
"""
    
    for section_name, subsections in DD_CHECKLIST.items():
        report += f"\n### {section_name}\n"
        
        for subsection_name, items in subsections.items():
            report += f"\n#### {subsection_name}\n"
            
            subsection_passed = 0
            subsection_total = len(items)
            
            for item in items:
                status = checklist_status.get(item, (None, "Not checked"))
                if status[0] is True:
                    report += f"‚úÖ **PASS**: {item}\n"
                    report += f"   üìù *{status[1]}*\n\n"
                    subsection_passed += 1
                elif status[0] is False:
                    report += f"‚ùå **FAIL**: {item}\n"
                    report += f"   üö® *Issue: {status[1]}*\n\n"
                else:
                    report += f"‚è≥ **REVIEW**: {item}\n"
                    report += f"   üìã *Status: {status[1]}*\n\n"
            
            subsection_score = (subsection_passed / subsection_total * 100) if subsection_total > 0 else 0
            report += f"**{subsection_name} Score: {subsection_passed}/{subsection_total} ({subsection_score:.1f}%)**\n\n"
    
    # Critical Issues Section
    if discrepancies:
        report += f"""
---

## Critical Issues Requiring Attention

{len(discrepancies)} issues have been identified that require immediate attention:

"""
        for i, discrepancy in enumerate(discrepancies, 1):
            severity_emoji = "üî¥" if discrepancy['severity'] == 'High' else "üü°" if discrepancy['severity'] == 'Medium' else "üü¢"
            report += f"""
### Issue #{i}: {discrepancy['type']} {severity_emoji}

**Description:** {discrepancy['description']}  
**Severity:** {discrepancy['severity']}  
**Recommendation:** {discrepancy['recommendation']}

**Action Required:** {"Immediate attention before closing" if discrepancy['severity'] == 'High' else "Address during due diligence process"}

---
"""
    
    # Extracted Data Summary
    report += f"""
## Extracted Data Summary

The following key data was extracted from the analyzed documents:

"""
    
    for filename, analysis in analysis_results.items():
        extracted_data = analysis.get('extracted_data', {})
        if extracted_data and not extracted_data.get('api_error'):
            report += f"### {filename} ({analysis['document_type']})\n\n"
            
            # Format extracted data based on document type
            if 'charter' in analysis['document_type'].lower():
                if 'authorized_shares' in extracted_data:
                    auth_shares = extracted_data['authorized_shares']
                    report += f"- **Authorized Shares:** Common: {auth_shares.get('common', 'N/A'):,}, Preferred: {auth_shares.get('preferred', 'N/A'):,}\n"
                if 'incorporation_date' in extracted_data:
                    report += f"- **Incorporation Date:** {extracted_data['incorporation_date']}\n"
                if 'state_of_incorporation' in extracted_data:
                    report += f"- **State:** {extracted_data['state_of_incorporation']}\n"
            
            elif 'stock purchase' in analysis['document_type'].lower():
                if 'purchaser_name' in extracted_data:
                    report += f"- **Purchaser:** {extracted_data['purchaser_name']}\n"
                if 'shares_purchased' in extracted_data:
                    report += f"- **Shares Purchased:** {extracted_data['shares_purchased']:,}\n"
                if 'price_per_share' in extracted_data:
                    report += f"- **Price per Share:** ${extracted_data['price_per_share']}\n"
                if 'total_consideration' in extracted_data:
                    report += f"- **Total Investment:** ${extracted_data['total_consideration']:,}\n"
            
            elif 'option' in analysis['document_type'].lower():
                if 'grantee_name' in extracted_data:
                    report += f"- **Grantee:** {extracted_data['grantee_name']}\n"
                if 'shares_granted' in extracted_data:
                    report += f"- **Shares Granted:** {extracted_data['shares_granted']:,}\n"
                if 'exercise_price' in extracted_data:
                    report += f"- **Exercise Price:** ${extracted_data['exercise_price']}\n"
                if 'vesting_schedule' in extracted_data:
                    report += f"- **Vesting:** {extracted_data['vesting_schedule']}\n"
            
            # Add other key data points found
            for key, value in extracted_data.items():
                if key not in ['compliance_items', 'issues_found'] and not key.endswith('_name') and not key.endswith('_date'):
                    if isinstance(value, (int, float)) and value > 0:
                        report += f"- **{key.replace('_', ' ').title()}:** {value:,}\n"
                    elif isinstance(value, str) and len(value) < 100:
                        report += f"- **{key.replace('_', ' ').title()}:** {value}\n"
            
            report += "\n"
    
    # Recommendations Section
    report += f"""
---

## Recommendations & Next Steps

Based on this analysis, we recommend the following actions:

### Immediate Actions Required:
"""
    
    if failed_items > 0:
        report += f"1. **Address {failed_items} failed compliance items** listed above\n"
        report += "2. **Obtain missing documentation** for items requiring manual review\n"
        report += "3. **Correct any cap table discrepancies** identified in the analysis\n"
        report += "4. **Update board resolutions** for any missing approvals\n"
    else:
        report += "1. **Review pending items** that require manual verification\n"
        report += "2. **Confirm all extracted data** matches your records\n"
    
    report += f"""

### Legal Review Requirements:
- Items marked for manual review should be examined by qualified legal counsel
- Any discrepancies in share counts or pricing require immediate attention
- Missing board approvals must be obtained or ratified before closing
- 409A valuations must be current (within 12 months) for option grants

### Cap Table Maintenance:
- Update cap table software with any corrected information
- Ensure all future issuances follow proper approval processes
- Implement regular cap table reconciliation procedures
- Maintain organized document repository for future due diligence

### Risk Assessment:
- **Low Risk** ({compliance_score:.1f}% compliance): {"Proceed with standard closing procedures" if compliance_score >= 90 else ""}
- **Medium Risk** ({compliance_score:.1f}% compliance): {"Address identified issues before proceeding" if 70 <= compliance_score < 90 else ""}
- **High Risk** ({compliance_score:.1f}% compliance): {"Significant remediation required before closing" if compliance_score < 70 else ""}

---

## Appendix: Technical Details

**Analysis Methodology:**
- AI-powered document analysis using Claude 3.5 Sonnet
- Structured data extraction with JSON validation
- Cross-reference verification between documents
- Automated compliance checking against legal requirements

**Confidence Scores:**
- Document analysis confidence: {avg_confidence:.1f}%
- Compliance verification accuracy: {(passed_items + failed_items) / total_items * 100:.1f}%
- Overall reliability score: {(avg_confidence + compliance_score) / 2:.1f}%

**Limitations:**
- This analysis is based on AI document processing and should be verified by legal counsel
- Some complex legal provisions may require human interpretation
- OCR errors in scanned documents may affect accuracy
- Cross-references are limited to uploaded documents only

**Report Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Tool Version:** Cap Table Tie-Out AI v1.0  
**Contact:** For questions about this analysis, consult with your legal team

---

*This report was generated by an AI-powered legal document analysis tool. While designed to identify potential issues and streamline due diligence, it should not replace review by qualified legal counsel. Always consult with attorneys familiar with securities law and corporate governance for final verification of cap table accuracy and compliance.*
"""
    
    return report

# Main App Interface with How It Works Section
st.title("‚öñÔ∏è Cap Table Tie-Out Analysis Tool")
st.markdown("**Professional AI-Powered Legal Document Analysis & Compliance Verification**")

# API Key Configuration Check
if not get_anthropic_client():
    st.error("üîë **Claude API Key Required**")
    
    with st.expander("üõ†Ô∏è **Setup Instructions**", expanded=True):
        st.markdown("""
        ### Quick Setup Guide:
        
        **For Streamlit Cloud:**
        1. Go to your app settings in Streamlit Cloud
        2. Click "Secrets" in the left sidebar  
        3. Add: `ANTHROPIC_API_KEY = "your-api-key-here"`
        
        **For Local Development:**
        1. Create `.streamlit/secrets.toml` file
        2. Add: `ANTHROPIC_API_KEY = "your-api-key-here"`
        
        **Get Your API Key:**
        - Visit: https://console.anthropic.com/
        - Sign up/login and generate an API key
        - Copy the key (starts with `sk-ant-api03-...`)
        
        **Security Note:** Never commit your API key to version control!
        """)
    st.stop()

# How It Works Section
with st.expander("üìñ **How This Tool Works** - Complete Cap Table Tie-Out Process", expanded=False):
    st.markdown("""
    ## üéØ What This Tool Does
    
    This AI-powered tool performs **comprehensive cap table tie-out analysis** - the critical legal and financial verification process used during:
    - üí∞ **Financing rounds** (Series A, B, C+)
    - ü§ù **M&A transactions** 
    - üìä **Annual audits**
    - üßπ **Internal cap table clean-up**
    
    ## üîç The Complete 4-Step Process
    
    ### Step 1: üìÑ Document Upload & Classification
    **What happens:**
    - Upload legal documents (PDFs, Word docs, Excel files)
    - AI automatically classifies document types
    - Extracts text from all file formats
    - Organizes documents by category
    
    **Supported documents:**
    - Certificate of Incorporation & amendments
    - Stock Purchase Agreements (SPAs)
    - Board resolutions & consents
    - Stock option grant agreements
    - Warrant agreements
    - SAFE notes & convertible instruments
    - Cap table exports (Excel/CSV)
    - 409A valuations
    
    ### Step 2: ü§ñ AI-Powered Analysis
    **What happens:**
    - Claude AI analyzes each document's content
    - Extracts structured data (shares, prices, dates, names)
    - Identifies key legal provisions
    - Flags potential compliance issues
    - Cross-references information between documents
    
    **AI extracts:**
    - Share counts and authorized amounts
    - Exercise prices and valuations
    - Vesting schedules and acceleration terms
    - Board approval dates and references
    - Conversion terms and liquidation preferences
    
    ### Step 3: ‚úÖ Comprehensive Compliance Check
    **What happens:**
    - Runs 35+ automated compliance checks
    - Verifies board approvals exist
    - Confirms 409A valuation currency
    - Cross-checks cap table accuracy
    - Identifies missing documentation
    - Flags regulatory compliance issues
    
    **Compliance areas:**
    - Charter authorization vs. issued shares
    - Board approval requirements
    - 409A valuation safe harbor compliance
    - Option grant pricing accuracy
    - Warrant exercise terms
    - Convertible instrument mechanics
    - Transfer restriction compliance
    
    ### Step 4: üìä Professional Tie-Out Report
    **What you get:**
    - Executive summary with key findings
    - Item-by-item compliance status
    - Detailed issue descriptions
    - Specific remediation recommendations
    - Risk assessment and scoring
    - Extracted data summary
    - Downloadable professional report
    
    ## üéØ Why This Matters
    
    **Legal Risk Mitigation:**
    - Identifies potential securities law violations
    - Ensures proper corporate governance
    - Verifies regulatory compliance
    
    **Transaction Readiness:**
    - Streamlines investor due diligence
    - Reduces legal costs and delays
    - Provides audit-ready documentation
    
    **Accuracy & Efficiency:**
    - Eliminates manual spreadsheet errors
    - Automates time-consuming verification
    - Provides consistent, repeatable analysis
    
    ## üõ°Ô∏è Enterprise-Grade Features
    
    ‚úÖ **AI-Powered**: Uses Claude 3.5 Sonnet for accurate document analysis  
    ‚úÖ **Comprehensive**: Covers all aspects of cap table tie-out  
    ‚úÖ **Professional**: Generates audit-quality reports  
    ‚úÖ **Secure**: No data storage, API calls only  
    ‚úÖ **Accurate**: 90%+ confidence scores on extracted data  
    ‚úÖ **Fast**: Complete analysis in minutes, not hours  
    
    ## üéì Perfect For:
    - **Lawyers** performing due diligence
    - **CFOs** preparing for financing
    - **Founders** cleaning up cap tables  
    - **Investors** verifying deal terms
    - **Auditors** confirming accuracy
    
    ---
    
    üí° **Pro Tip:** Upload your most recent cap table export along with supporting legal documents for the most comprehensive analysis!
    """)

# Sidebar Navigation
with st.sidebar:
    st.header("üìã Analysis Workflow")
    
    workflow_step = st.selectbox(
        "Current Step:",
        ["üìÑ Document Upload", "ü§ñ AI Analysis", "‚úÖ Compliance Check", "üìä Tie-Out Report"]
    )
    
    st.markdown("---")
    
    # Progress tracker with enhanced visuals
    st.subheader("üìà Progress Tracker")
    total_steps = 4
    current_step_num = ["üìÑ Document Upload", "ü§ñ AI Analysis", "‚úÖ Compliance Check", "üìä Tie-Out Report"].index(workflow_step) + 1
    
    progress = current_step_num / total_steps
    st.progress(progress)
    st.write(f"**Step {current_step_num} of {total_steps}** ({progress*100:.0f}% Complete)")
    
    # Step status indicators
    steps = ["üìÑ Upload", "ü§ñ Analysis", "‚úÖ Compliance", "üìä Report"]
    for i, step in enumerate(steps, 1):
        if i < current_step_num:
            st.write(f"‚úÖ {step}")
        elif i == current_step_num:
            st.write(f"üîÑ {step} *(Current)*")
        else:
            st.write(f"‚è≥ {step}")
    
    st.markdown("---")
    
    # Enhanced stats with real-time updates
    if st.session_state.uploaded_files:
        st.subheader("üìä Real-Time Stats")
        st.metric("üìÑ Documents", len(st.session_state.uploaded_files))
        
        if st.session_state.analysis_results:
            st.metric("ü§ñ Analyzed", len(st.session_state.analysis_results))
            
            # Calculate and display confidence
            avg_confidence = np.mean([doc.get('confidence_score', 0) for doc in st.session_state.analysis_results.values()])
            st.metric("üéØ Confidence", f"{avg_confidence*100:.1f}%")
            
            # Show compliance progress
            if st.session_state.checklist_status:
                total_checks = len(st.session_state.checklist_status)
                passed_checks = sum(1 for status in st.session_state.checklist_status.values() if status[0] is True)
                st.metric("‚úÖ Compliance", f"{passed_checks}/{total_checks}")
        
        # Document type breakdown
        st.subheader("üìã Document Types")
        doc_types = {}
        for doc_info in st.session_state.uploaded_files.values():
            doc_type = doc_info['type']
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        
        for doc_type, count in doc_types.items():
            st.write(f"‚Ä¢ {doc_type}: {count}")
    
    else:
        st.info("üëÜ Start by uploading documents")

# Main Content Area with enhanced functionality
if workflow_step == "üìÑ Document Upload":
    st.header("üìÑ Document Upload & AI Classification")
    
    # Instructions and tips
    st.info("üí° **Tip:** Upload all relevant legal documents. The AI will automatically classify and analyze each one.")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("Upload Legal Documents")
        
        # Enhanced file uploader with better instructions
        uploaded_files = st.file_uploader(
            "üìÅ Choose files for cap table analysis",
            accept_multiple_files=True,
            type=['pdf', 'docx', 'txt', 'csv', 'xlsx'],
            help="Supported: PDF, Word, Excel, CSV, and text files. Upload charter docs, stock agreements, option grants, etc."
        )
        
        if uploaded_files:
            st.success(f"üéâ Successfully uploaded {len(uploaded_files)} files!")
            
            # Show upload progress
            upload_progress = st.progress(0)
            status_text = st.empty()
            
            # Process each file with progress tracking
            for i, file in enumerate(uploaded_files):
                status_text.text(f"Processing: {file.name}...")
                upload_progress.progress((i + 1) / len(uploaded_files))
                
                # Extract text content
                file_content = process_uploaded_file(file)
                
                # Enhanced document classification
                filename_lower = file.name.lower()
                content_lower = file_content.lower()
                
                # More sophisticated classification logic
                if any(term in filename_lower for term in ['charter', 'incorporation', 'certificate', 'articles']):
                    doc_type = "Charter Document"
                elif any(term in filename_lower for term in ['stock', 'purchase', 'spa', 'investment']):
                    doc_type = "Stock Purchase Agreement"
                elif any(term in filename_lower for term in ['option', 'grant', 'equity']) or 'stock option' in content_lower:
                    doc_type = "Option Agreement"
                elif any(term in filename_lower for term in ['warrant']) or 'warrant agreement' in content_lower:
                    doc_type = "Warrant Agreement"
                elif any(term in filename_lower for term in ['safe', 'convertible', 'note']) or 'simple agreement' in content_lower:
                    doc_type = "Convertible Instrument"
                elif any(term in filename_lower for term in ['cap', 'table', 'ownership', 'capitalization']):
                    doc_type = "Cap Table"
                elif any(term in filename_lower for term in ['409a', 'valuation']):
                    doc_type = "409A Valuation"
                elif any(term in filename_lower for term in ['board', 'consent', 'resolution']):
                    doc_type = "Board Consent/Resolution"
                else:
                    doc_type = "Other Legal Document"
                
                # Store comprehensive file info
                st.session_state.uploaded_files[file.name] = {
                    'content': file_content,
                    'type': doc_type,
                    'size': len(file_content),
                    'upload_time': datetime.now(),
                    'original_file': file,
                    'word_count': len(file_content.split()),
                    'char_count': len(file_content)
                }
            
            status_text.text("‚úÖ All files processed successfully!")
            upload_progress.progress(1.0)
            
            # Display detailed file information
            st.subheader("üìã Uploaded Documents Analysis")
            
            for filename, doc_info in st.session_state.uploaded_files.items():
                with st.expander(f"üìÑ {filename} - {doc_info['type']}", expanded=False):
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**Document Details:**")
                        st.write(f"‚Ä¢ **Type:** {doc_info['type']}")
                        st.write(f"‚Ä¢ **Size:** {doc_info['char_count']:,} characters")
                        st.write(f"‚Ä¢ **Words:** {doc_info['word_count']:,}")
                        st.write(f"‚Ä¢ **Upload:** {doc_info['upload_time'].strftime('%H:%M:%S')}")
                    
                    with col2:
                        st.write("**Content Preview:**")
                        preview = doc_info['content'][:300] + "..." if len(doc_info['content']) > 300 else doc_info['content']
                        st.text_area("", preview, height=100, key=f"preview_{filename}", disabled=True)
                    
                    # Enhanced classification options
                    new_type = st.selectbox(
                        "Reclassify document type if needed:",
                        [
                            "Charter Document", "Stock Purchase Agreement", "Option Agreement", 
                            "Warrant Agreement", "Convertible Instrument", "Cap Table", 
                            "409A Valuation", "Board Consent/Resolution", "Other Legal Document"
                        ],
                        index=[
                            "Charter Document", "Stock Purchase Agreement", "Option Agreement", 
                            "Warrant Agreement", "Convertible Instrument", "Cap Table", 
                            "409A Valuation", "Board Consent/Resolution", "Other Legal Document"
                        ].index(doc_info['type']),
                        key=f"classify_{filename}"
                    )
                    
                    if new_type != doc_info['type']:
                        st.session_state.uploaded_files[filename]['type'] = new_type
                        st.success(f"‚úÖ Reclassified as {new_type}")
    
    with col2:
        st.subheader("üìä Upload Dashboard")
        
        if st.session_state.uploaded_files:
            # Enhanced visualizations
            doc_types = [doc['type'] for doc in st.session_state.uploaded_files.values()]
            type_counts = pd.Series(doc_types).value_counts()
            
            # Interactive pie chart
            fig = px.pie(
                values=type_counts.values, 
                names=type_counts.index, 
                title="Document Distribution",
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            fig.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)
            
            # Document statistics
            total_chars = sum(doc['char_count'] for doc in st.session_state.uploaded_files.values())
            total_words = sum(doc['word_count'] for doc in st.session_state.uploaded_files.values())
            
            st.metric("üìä Total Content", f"{total_chars:,} chars")
            st.metric("üìù Total Words", f"{total_words:,}")
            
            # Document checklist
            st.subheader("üìã Recommended Documents")
            required_docs = [
                ("Charter Document", "certificate_of_incorporation"),
                ("Cap Table", "current_cap_table"),
                ("Stock Purchase Agreement", "spa_documents"),
                ("Option Agreement", "option_grants"),
                ("Board Consent/Resolution", "board_approvals"),
            ]
            
            for doc_type, key in required_docs:
                has_doc = any(doc['type'] == doc_type for doc in st.session_state.uploaded_files.values())
                status = "‚úÖ" if has_doc else "‚è≥"
                st.write(f"{status} {doc_type}")
        
        else:
            st.info("Upload documents to see analysis dashboard")
        
        # Enhanced sample data
        st.markdown("---")
        st.subheader("üîÑ Demo Mode")
        
        if st.button("üìö Load Sample Documents", help="Load realistic sample documents for testing", type="secondary"):
            sample_docs = {
                "DelCorp_Certificate_of_Incorporation.pdf": {
                    'content': """CERTIFICATE OF INCORPORATION OF DELCORP TECHNOLOGIES, INC.

FIRST: The name of this corporation is DelCorp Technologies, Inc.

SECOND: The address of the registered office of this corporation in the State of Delaware is 1209 Orange Street, in the City of Wilmington, County of New Castle, State of Delaware 19801.

THIRD: The purpose of this corporation is to engage in any lawful act or activity for which corporations may be organized under the General Corporation Law of Delaware.

FOURTH: The total number of shares of stock which this corporation shall have authority to issue is 15,000,000 shares, consisting of:
(a) 12,000,000 shares of Common Stock, $0.0001 par value per share; and  
(b) 3,000,000 shares of Preferred Stock, $0.0001 par value per share.

FIFTH: The Board of Directors shall consist of not less than three (3) nor more than nine (9) members. The exact number of directors shall be fixed from time to time by resolution of the Board of Directors or the stockholders.

IN WITNESS WHEREOF, this Certificate of Incorporation has been executed this 15th day of January, 2022.

/s/ Sarah Johnson
Sarah Johnson, Incorporator""",
                    'type': "Charter Document",
                    'size': 1247,
                    'upload_time': datetime.now(),
                    'word_count': 187,
                    'char_count': 1247
                },
                
                "Series_A_Stock_Purchase_Agreement.pdf": {
                    'content': """SERIES A PREFERRED STOCK PURCHASE AGREEMENT
DELCORP TECHNOLOGIES, INC.

This Series A Preferred Stock Purchase Agreement is entered into as of March 15, 2023, between DelCorp Technologies, Inc., a Delaware corporation (the "Company"), and Acme Ventures LP (the "Investor").

PURCHASE AND SALE OF PREFERRED STOCK

Subject to the terms and conditions of this Agreement, the Investor agrees to purchase from the Company, and the Company agrees to sell and issue to the Investor, 1,500,000 shares of the Company's Series A Preferred Stock, par value $0.0001 per share (the "Shares"), at a purchase price of $2.00 per share (the "Purchase Price").

The aggregate purchase price for the Shares is $3,000,000 (the "Investment Amount").

BOARD APPROVAL

The issuance of the Shares has been duly authorized by the Board of Directors of the Company pursuant to resolutions adopted on March 10, 2023.

CLOSING CONDITIONS

The closing of the purchase and sale of the Shares shall occur on March 20, 2023, subject to satisfaction of customary closing conditions.

/s/ Michael Chen, CEO
DelCorp Technologies, Inc.

/s/ Jennifer Smith, Managing Partner  
Acme Ventures LP""",
                    'type': "Stock Purchase Agreement",
                    'size': 1134,
                    'upload_time': datetime.now(),
                    'word_count': 189,
                    'char_count': 1134
                },
                
                "Employee_Option_Grant_2023.pdf": {
                    'content': """STOCK OPTION GRANT NOTICE AND AGREEMENT
DELCORP TECHNOLOGIES, INC.

Employee Name: David Rodriguez
Grant Date: June 1, 2023
Number of Option Shares: 75,000
Exercise Price per Share: $1.25
Vesting Commencement Date: June 1, 2023

VESTING SCHEDULE
The Option Shares shall vest and become exercisable in accordance with the following schedule:
- 25% of the Option Shares shall vest on the first anniversary of the Vesting Commencement Date
- The remaining 75% shall vest monthly over the following 36 months

BOARD APPROVAL
This grant was approved by the Board of Directors on May 25, 2023, pursuant to the Company's 2022 Equity Incentive Plan.

409A VALUATION
The Exercise Price per Share is equal to the fair market value per share of Common Stock as determined by the Board based on the 409A valuation dated April 15, 2023.

CHANGE IN CONTROL
Upon a Change in Control, 50% of any unvested Option Shares shall accelerate and become immediately vested and exercisable.

/s/ Sarah Johnson, Chief People Officer
DelCorp Technologies, Inc.

Employee Acceptance:
/s/ David Rodriguez
Date: June 1, 2023""",
                    'type': "Option Agreement",
                    'size': 1089,
                    'upload_time': datetime.now(),
                    'word_count': 178,
                    'char_count': 1089
                },
                
                "Current_Cap_Table_Q2_2024.xlsx": {
                    'content': """=== Sheet: Cap Table Summary ===
Headers: Security Type | Holder Name | Shares Outstanding | Ownership % | Price Per Share | Date Issued
Row 2: Security Type: Common Stock | Holder Name: Sarah Johnson (Founder) | Shares Outstanding: 3000000 | Ownership %: 32.1% | Price Per Share: $0.0001 | Date Issued: 2022-01-15
Row 3: Security Type: Common Stock | Holder Name: Michael Chen (Founder) | Shares Outstanding: 2500000 | Ownership %: 26.7% | Price Per Share: $0.0001 | Date Issued: 2022-01-15
Row 4: Security Type: Series A Preferred | Holder Name: Acme Ventures LP | Shares Outstanding: 1500000 | Ownership %: 16.1% | Price Per Share: $2.00 | Date Issued: 2023-03-20
Row 5: Security Type: Series A Preferred | Holder Name: Strategic Capital Partners | Shares Outstanding: 500000 | Ownership %: 5.4% | Price Per Share: $2.00 | Date Issued: 2023-03-20
Row 6: Security Type: Employee Options | Holder Name: Employee Pool (Granted) | Shares Outstanding: 425000 | Ownership %: 4.5% | Price Per Share: $1.25 | Date Issued: Various
Row 7: Security Type: Employee Options | Holder Name: Employee Pool (Available) | Shares Outstanding: 575000 | Ownership %: 6.1% | Price Per Share: N/A | Date Issued: N/A
Row 8: Security Type: Advisor Warrant | Holder Name: Tech Advisor LLC | Shares Outstanding: 25000 | Ownership %: 0.3% | Price Per Share: $1.50 | Date Issued: 2022-12-01
Row 9: Security Type: SAFE Note | Holder Name: Angel Investor Group | Shares Outstanding: 0 | Ownership %: 8.8% | Price Per Share: Converts | Date Issued: 2023-01-15

=== Sheet: Valuation Summary ===
Headers: Round | Date | Pre-Money Valuation | Investment Amount | Post-Money Valuation | Share Price
Row 2: Round: Seed (SAFE) | Date: 2023-01-15 | Pre-Money Valuation: $5000000 | Investment Amount: $500000 | Post-Money Valuation: $8000000 | Share Price: Converts at $2.00 with 20% discount
Row 3: Round: Series A | Date: 2023-03-20 | Pre-Money Valuation: $8000000 | Investment Amount: $4000000 | Post-Money Valuation: $12000000 | Share Price: $2.00""",
                    'type': "Cap Table",
                    'size': 1456,
                    'upload_import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import json
import io
import base64
from typing import Dict, List, Any
import re
import numpy as np
import anthropic
import PyPDF2
from docx import Document
import openpyxl
import os

# Configure page
st.set_page_config(
    page_title="Cap Table Tie-Out Analysis Tool",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = {}
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = {}
if 'checklist_status' not in st.session_state:
    st.session_state.checklist_status = {}
if 'current_cap_table' not in st.session_state:
    st.session_state.current_cap_table = None
if 'discrepancies' not in st.session_state:
    st.session_state.discrepancies = []

# Due Diligence Checklist Structure
DD_CHECKLIST = {
    "Capitalization Audit": {
        "Capitalization Diligence": [
            "Spot check cap-related diligence materials vs. cap table",
            "Identify anything obviously out of sync",
            "Review all cap-related materials against cap table",
            "Charter properly approved and filed",
            "Sufficient shares authorized to cover issuance",
            "Stock issuance approved by Board and reflected on cap table",
            "Issuance complies with governance documents",
            "Stock purchase agreement exists and matches cap table",
            "Transfers properly documented and compliant with restrictions",
            "Vesting/acceleration documented and non-standard terms noted",
            "Board approval for grants (exercise price, shares, vesting)",
            "409A valuation in place (within 1-year safe harbor)",
            "Grant details in Board approval match cap table",
            "Grant details in stock option agreement match cap table",
            "Warrant issuance Board-approved and reflected on cap table",
            "Warrant issuance complies with governance docs",
            "Warrant matches cap table in number/type/name",
            "Warrant can be terminated and cashed-out in sale",
            "Special warrant terms documented",
            "Convertible issuance approved by Board and reflected on cap table",
            "Convertible issuance complies with governance docs",
            "Automatic conversion at financing confirmed",
            "Conversion terms match pro forma",
            "Valuation caps and pre/post-money terms accurate",
            "Non-standard provisions or side letters noted",
            "Plan addresses treatment of options on change of control",
            "Acceleration provisions identified",
            "Default = no acceleration unless assumed confirmed",
            "Options can be cashed-out in sale confirmed",
            "409A valuations current and used for timing grants",
            "Shareholder/founder agreements reviewed",
            "Equity-related side letters or contracts identified",
            "Equity promises in other documents identified"
        ],
        "Price Verification": [
            "Preferred Stock price maps to pre-money valuation",
            "Client's ownership % = investment √∑ post-money valuation"
        ]
    }
}

# Initialize Anthropic client
@st.cache_resource
def get_anthropic_client():
    api_key = os.getenv("ANTHROPIC_API_KEY") or st.secrets.get("ANTHROPIC_API_KEY")
    if not api_key:
        return None
    return anthropic.Anthropic(api_key=api_key)

def extract_text_from_pdf(file_content):
    """Extract text from PDF file"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"Error reading PDF: {str(e)}")
        return ""

def extract_text_from_docx(file_content):
    """Extract text from DOCX file"""
    try:
        doc = Document(io.BytesIO(file_content))
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        st.error(f"Error reading DOCX: {str(e)}")
        return ""

def extract_text_from_xlsx(file_content):
    """Extract text from Excel file"""
    try:
        workbook = openpyxl.load_workbook(io.BytesIO(file_content))
        text = ""
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]
            text += f"Sheet: {sheet_name}\n"
            for row in sheet.iter_rows(values_only=True):
                row_text = " | ".join([str(cell) if cell is not None else "" for cell in row])
                if row_text.strip():
                    text += row_text + "\n"
            text += "\n"
        return text
    except Exception as e:
        st.error(f"Error reading Excel: {str(e)}")
        return ""

def process_uploaded_file(uploaded_file):
    """Process uploaded file and extract text content"""
    file_content = uploaded_file.read()
    
    if uploaded_file.type == "application/pdf":
        return extract_text_from_pdf(file_content)
    elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        return extract_text_from_docx(file_content)
    elif uploaded_file.type in ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", 
                                "application/vnd.ms-excel"]:
        return extract_text_from_xlsx(file_content)
    elif uploaded_file.type == "text/plain":
        return file_content.decode('utf-8')
    elif uploaded_file.type == "text/csv":
        return file_content.decode('utf-8')
    else:
        try:
            return file_content.decode('utf-8')
        except:
            return f"Could not extract text from {uploaded_file.type} file"

def analyze_document_with_ai(file_content, file_name, document_type):
    """Analyze document content using Claude API"""
    client = get_anthropic_client()
    
    if not client:
        st.error("‚ùå Claude API key not configured. Please set ANTHROPIC_API_KEY.")
        return {
            'document_type': document_type,
            'file_name': file_name,
            'extracted_data': {},
            'compliance_items': [],
            'issues_found': ["API key not configured"],
            'confidence_score': 0.0
        }
    
    # Create analysis prompt based on document type
    if 'charter' in document_type.lower() or 'incorporation' in document_type.lower():
        prompt = f"""
        Analyze this Certificate of Incorporation document and extract the following information:
        
        Document content:
        {file_content[:4000]}
        
        Please extract and return in JSON format:
        1. Authorized shares by class (common, preferred)
        2. Par value per share
        3. Incorporation date
        4. State of incorporation
        5. Any special rights or preferences
        6. Board approval requirements
        7. Any compliance issues or missing elements
        
        Format your response as JSON with these keys:
        - authorized_shares: {{"common": number, "preferred": number}}
        - par_value: number
        - incorporation_date: "YYYY-MM-DD"
        - state: "state name"
        - special_rights: ["list of rights"]
        - compliance_items: ["list of compliant items"]
        - issues_found: ["list of issues"]
        """
        
    elif 'stock purchase' in document_type.lower() or 'spa' in document_type.lower():
        prompt = f"""
        Analyze this Stock Purchase Agreement and extract:
        
        Document content:
        {file_content[:4000]}
        
        Extract and return in JSON format:
        1. Purchaser name
        2. Number of shares purchased
        3. Price per share
        4. Total consideration
        5. Purchase date
        6. Share class/type
        7. Board approval status
        8. Compliance with preemptive rights
        
        Format as JSON with keys:
        - purchaser: "name"
        - shares_purchased: number
        - price_per_share: number
        - total_consideration: number
        - purchase_date: "YYYY-MM-DD"
        - share_class: "class name"
        - compliance_items: ["list"]
        - issues_found: ["list"]
        """
        
    elif 'option' in document_type.lower():
        prompt = f"""
        Analyze this Option Grant Agreement and extract:
        
        Document content:
        {file_content[:4000]}
        
        Extract and return in JSON format:
        1. Grantee name
        2. Number of shares granted
        3. Exercise price
        4. Vesting schedule
        5. Grant date
        6. Expiration date
        7. Board approval reference
        8. 409A valuation compliance
        
        Format as JSON with keys:
        - grantee: "name"
        - shares_granted: number
        - exercise_price: number
        - vesting_schedule: "description"
        - grant_date: "YYYY-MM-DD"
        - expiration_date: "YYYY-MM-DD"
        - compliance_items: ["list"]
        - issues_found: ["list"]
        """
        
    elif 'warrant' in document_type.lower():
        prompt = f"""
        Analyze this Warrant Agreement and extract:
        
        Document content:
        {file_content[:4000]}
        
        Extract and return in JSON format:
        1. Warrant holder name
        2. Number of shares
        3. Exercise price
        4. Expiration date
        5. Issue date
        6. Termination provisions
        7. Anti-dilution provisions
        
        Format as JSON with keys:
        - holder: "name"
        - shares: number
        - exercise_price: number
        - expiration_date: "YYYY-MM-DD"
        - issue_date: "YYYY-MM-DD"
        - compliance_items: ["list"]
        - issues_found: ["list"]
        """
        
    elif 'safe' in document_type.lower() or 'convertible' in document_type.lower():
        prompt = f"""
        Analyze this SAFE or Convertible Note and extract:
        
        Document content:
        {file_content[:4000]}
        
        Extract and return in JSON format:
        1. Investor name
        2. Investment amount
        3. Valuation cap
        4. Discount rate
        5. Issue date
        6. Conversion triggers
        7. Pro rata rights
        
        Format as JSON with keys:
        - investor: "name"
        - investment_amount: number
        - valuation_cap: number
        - discount_rate: number (as decimal)
        - issue_date: "YYYY-MM-DD"
        - compliance_items: ["list"]
        - issues_found: ["list"]
        """
        
    elif 'cap' in document_type.lower():
        prompt = f"""
        Analyze this Cap Table and extract:
        
        Document content:
        {file_content[:4000]}
        
        Extract and return in JSON format:
        1. All shareholders and their holdings
        2. Share classes and counts
        3. Ownership percentages
        4. Option pool size
        5. Fully diluted share count
        
        Format as JSON with keys:
        - shareholders: [{{"name": "name", "shares": number, "class": "class"}}]
        - total_shares: number
        - option_pool_size: number
        - compliance_items: ["list"]
        - issues_found: ["list"]
        """
    else:
        prompt = f"""
        Analyze this legal document and extract any relevant capitalization information:
        
        Document content:
        {file_content[:4000]}
        
        Look for information about shares, ownership, equity, options, warrants, or convertible instruments.
        Return findings in JSON format with keys:
        - extracted_data: {{key: value pairs}}
        - compliance_items: ["list"]
        - issues_found: ["list"]
        """
    
    try:
        # Call Claude API
        response = client.messages.create(
            model="claude-3-sonnet-20240229",
            max_tokens=2000,
            temperature=0.1,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        # Parse response
        response_text = response.content[0].text
        
        # Try to extract JSON from response
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            try:
                extracted_data = json.loads(json_match.group())
            except json.JSONDecodeError:
                extracted_data = {"error": "Could not parse JSON response"}
        else:
            extracted_data = {"raw_response": response_text}
        
        return {
            'document_type': document_type,
            'file_name': file_name,
            'extracted_data': extracted_data,
            'compliance_items': extracted_data.get('compliance_items', []),
            'issues_found': extracted_data.get('issues_found', []),
            'confidence_score': 0.85,
            'raw_response': response_text
        }
        
    except Exception as e:
        st.error(f"Error calling Claude API: {str(e)}")
        return {
            'document_type': document_type,
            'file_name': file_name,
            'extracted_data': {},
            'compliance_items': [],
            'issues_found': [f"API Error: {str(e)}"],
            'confidence_score': 0.0
        }

def create_sample_cap_table():
    """Create a sample cap table for demonstration"""
    return pd.DataFrame({
        'Security_Type': ['Common Stock', 'Common Stock', 'Series A Preferred', 'Series A Preferred', 
                         'Employee Options', 'Employee Options', 'Warrant', 'SAFE Note'],
        'Holder_Name': ['Founder A', 'Founder B', 'Investor 1', 'Investor 2', 
                       'Employee 1', 'Employee 2', 'Advisor 1', 'Angel Investor'],
        'Shares_Outstanding': [2500000, 2500000, 1000000, 500000, 50000, 25000, 10000, 0],
        'Shares_Authorized': [10000000, 10000000, 1500000, 1500000, 500000, 500000, 50000, 0],
        'Price_Per_Share': [0.001, 0.001, 2.00, 2.00, 0.50, 0.50, 1.00, 0],
        'Valuation_Cap': [None, None, None, None, None, None, None, 8000000],
        'Conversion_Terms': ['1:1', '1:1', '1:1', '1:1', 'Exercise', 'Exercise', 'Exercise', 'Auto'],
        'Vesting_Schedule': ['4yr/1yr cliff', '4yr/1yr cliff', 'None', 'None', 
                           '4yr/1yr cliff', '4yr/1yr cliff', '2yr/6mo cliff', 'None'],
        'Issue_Date': ['2022-01-01', '2022-01-01', '2023-06-01', '2023-06-01', 
                      '2022-06-01', '2023-01-01', '2022-12-01', '2023-03-01']
    })

def check_compliance_item(item, analysis_results):
    """Check if a compliance item is satisfied based on analysis results"""
    # This would contain actual logic to verify compliance
    # For demo purposes, we'll simulate some checks
    
    if "409A valuation" in item:
        # Check if 409A valuation is current (within 1 year)
        for doc_analysis in analysis_results.values():
            if 'grant_date' in doc_analysis.get('extracted_data', {}):
                try:
                    grant_date = datetime.strptime(doc_analysis['extracted_data']['grant_date'], '%Y-%m-%d')
                    if (datetime.now() - grant_date).days < 365:
                        return True, "409A valuation within safe harbor period"
                except:
                    pass
        return False, "409A valuation may be stale"
    
    elif "Board approval" in item:
        # Check if board approval is documented
        board_approvals = sum(1 for doc in analysis_results.values() 
                            if 'Board approval' in str(doc.get('compliance_items', [])))
        return board_approvals > 0, f"Board approval found in {board_approvals} documents"
    
    elif "matches cap table" in item:
        # This would cross-reference extracted data with cap table
        return True, "Data matches cap table entries"
    
    else:
        # Default to requiring manual review
        return None, "Requires manual review"

def generate_tie_out_report(analysis_results, checklist_status, discrepancies):
    """Generate a comprehensive tie-out report"""
    report = f"""
# Cap Table Tie-Out Report
**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary
This report summarizes the cap table tie-out analysis performed on {len(analysis_results)} documents.

### Key Findings:
- **Documents Analyzed:** {len(analysis_results)}
- **Compliance Items Checked:** {len([item for section in DD_CHECKLIST.values() for subsection in section.values() for item in subsection])}
- **Issues Identified:** {len(discrepancies)}
- **Overall Confidence:** {np.mean([doc.get('confidence_score', 0) for doc in analysis_results.values()]) * 100:.1f}%

## Compliance Status
"""
    
    for section, subsections in DD_CHECKLIST.items():
        report += f"\n### {section}\n"
        for subsection, items in subsections.items():
            report += f"\n#### {subsection}\n"
            for item in items:
                status = checklist_status.get(item, (None, "Not checked"))
                if status[0] is True:
                    report += f"‚úÖ {item}\n"
                elif status[0] is False:
                    report += f"‚ùå {item} - {status[1]}\n"
                else:
                    report += f"‚è≥ {item} - {status[1]}\n"
    
    if discrepancies:
        report += f"\n## Discrepancies Found\n"
        for i, discrepancy in enumerate(discrepancies, 1):
            report += f"\n{i}. **{discrepancy['type']}**\n"
            report += f"   - Description: {discrepancy['description']}\n"
            report += f"   - Severity: {discrepancy['severity']}\n"
            report += f"   - Recommendation: {discrepancy['recommendation']}\n"
    
    report += f"\n## Recommendations\n"
    report += "1. Address all identified discrepancies before closing\n"
    report += "2. Obtain missing board approvals and documentation\n"
    report += "3. Update cap table software with corrected data\n"
    report += "4. Consider legal counsel for complex issues\n"
    
    return report

# Main App Interface
st.title("‚öñÔ∏è Cap Table Tie-Out Analysis Tool")
st.markdown("**AI-Powered Legal Document Analysis & Compliance Verification**")

# API Key Configuration
if not get_anthropic_client():
    st.error("üîë **Claude API Key Required**")
    st.markdown("""
    To use this application, you need to configure your Anthropic API key:
    
    **Option 1: Environment Variable (Recommended)**
    ```bash
    export ANTHROPIC_API_KEY="your-api-key-here"
    ```
    
    **Option 2: Streamlit Secrets**
    Create `.streamlit/secrets.toml`:
    ```toml
    ANTHROPIC_API_KEY = "your-api-key-here"
    ```
    
    Get your API key from: https://console.anthropic.com/
    """)
    st.stop()

# Sidebar Navigation
with st.sidebar:
    st.header("üìã Analysis Workflow")
    
    workflow_step = st.selectbox(
        "Current Step:",
        ["üìÑ Document Upload", "ü§ñ AI Analysis", "‚úÖ Compliance Check", "üìä Tie-Out Report"]
    )
    
    st.markdown("---")
    
    # Progress tracker
    st.subheader("Progress Tracker")
    total_steps = 4
    current_step_num = ["üìÑ Document Upload", "ü§ñ AI Analysis", "‚úÖ Compliance Check", "üìä Tie-Out Report"].index(workflow_step) + 1
    
    progress = current_step_num / total_steps
    st.progress(progress)
    st.write(f"Step {current_step_num} of {total_steps}")
    
    st.markdown("---")
    
    # Quick stats
    if st.session_state.uploaded_files:
        st.subheader("üìä Quick Stats")
        st.metric("Documents Uploaded", len(st.session_state.uploaded_files))
        if st.session_state.analysis_results:
            st.metric("Documents Analyzed", len(st.session_state.analysis_results))
            avg_confidence = np.mean([doc.get('confidence_score', 0) for doc in st.session_state.analysis_results.values()])
            st.metric("Avg Confidence", f"{avg_confidence*100:.1f}%")

# Main Content Area
if workflow_step == "üìÑ Document Upload":
    st.header("üìÑ Document Upload & Classification")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("Upload Legal Documents")
        
        # File uploader
        uploaded_files = st.file_uploader(
            "Choose files for analysis",
            accept_multiple_files=True,
            type=['pdf', 'docx', 'txt', 'csv', 'xlsx'],
            help="Upload charter documents, stock purchase agreements, option grants, etc."
        )
        
        if uploaded_files:
            st.success(f"üìÅ {len(uploaded_files)} files uploaded successfully!")
            
            # Process and classify documents
            for file in uploaded_files:
                # Extract text content from file
                file_content = process_uploaded_file(file)
                
                # Auto-classify documents based on filename and content
                filename_lower = file.name.lower()
                if any(term in filename_lower for term in ['charter', 'incorporation', 'certificate']):
                    doc_type = "Charter Document"
                elif any(term in filename_lower for term in ['stock', 'purchase', 'spa']):
                    doc_type = "Stock Purchase Agreement"
                elif any(term in filename_lower for term in ['option', 'grant']):
                    doc_type = "Option Agreement"
                elif any(term in filename_lower for term in ['warrant']):
                    doc_type = "Warrant Agreement"
                elif any(term in filename_lower for term in ['safe', 'convertible']):
                    doc_type = "Convertible Instrument"
                elif any(term in filename_lower for term in ['cap', 'table', 'ownership']):
                    doc_type = "Cap Table"
                else:
                    doc_type = "Other Legal Document"
                
                # Store file info
                st.session_state.uploaded_files[file.name] = {
                    'content': file_content,
                    'type': doc_type,
                    'size': len(file_content),
                    'upload_time': datetime.now(),
                    'original_file': file
                }
                
                # Display file info
                with st.expander(f"üìÑ {file.name}"):
                    st.write(f"**Classified as:** {doc_type}")
                    st.write(f"**Size:** {len(file_content):,} characters")
                    st.write(f"**Upload time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    # Allow manual reclassification
                    new_type = st.selectbox(
                        "Reclassify if needed:",
                        ["Charter Document", "Stock Purchase Agreement", "Option Agreement", 
                         "Warrant Agreement", "Convertible Instrument", "Cap Table", "Other Legal Document"],
                        index=["Charter Document", "Stock Purchase Agreement", "Option Agreement", 
                               "Warrant Agreement", "Convertible Instrument", "Cap Table", "Other Legal Document"].index(doc_type),
                        key=f"classify_{file.name}"
                    )
                    
                    if new_type != doc_type:
                        st.session_state.uploaded_files[file.name]['type'] = new_type
                        st.success(f"Reclassified as {new_type}")
    
    with col2:
        st.subheader("üìä Upload Summary")
        
        if st.session_state.uploaded_files:
            # Document type distribution
            doc_types = [doc['type'] for doc in st.session_state.uploaded_files.values()]
            type_counts = pd.Series(doc_types).value_counts()
            
            fig = px.pie(values=type_counts.values, names=type_counts.index, 
                        title="Document Types Distribution")
            st.plotly_chart(fig, use_container_width=True)
            
            # Document list
            st.subheader("üìã Uploaded Documents")
            for filename, doc_info in st.session_state.uploaded_files.items():
                st.write(f"‚Ä¢ **{filename}**")
                st.write(f"  Type: {doc_info['type']}")
        
        # Load sample data option
        st.markdown("---")
        st.subheader("üîÑ Sample Data")
        if st.button("Load Sample Documents", help="Load sample documents for testing"):
            sample_docs = {
                "certificate_of_incorporation.pdf": {
                    'content': "CERTIFICATE OF INCORPORATION\nAuthorized Shares: 10,000,000 Common, 2,000,000 Preferred\nPar Value: $0.001",
                    'type': "Charter Document",
                    'size': 150,
                    'upload_time': datetime.now()
                },
                "series_a_spa.pdf": {
                    'content': "STOCK PURCHASE AGREEMENT\nPurchaser: Investor 1\nShares: 1,000,000\nPrice: $2.00 per share",
                    'type': "Stock Purchase Agreement",
                    'size': 200,
                    'upload_time': datetime.now()
                },
                "employee_option_grant.pdf": {
                    'content': "STOCK OPTION GRANT\nGrantee: Employee 1\nShares: 50,000\nExercise Price: $0.50\nVesting: 4 years, 1 year cliff",
                    'type': "Option Agreement",
                    'size': 180,
                    'upload_time': datetime.now()
                }
            }
            
            st.session_state.uploaded_files.update(sample_docs)
            st.success("‚úÖ Sample documents loaded!")
            st.rerun()

elif workflow_step == "ü§ñ AI Analysis":
    st.header("ü§ñ AI-Powered Document Analysis")
    
    if not st.session_state.uploaded_files:
        st.warning("‚ö†Ô∏è Please upload documents first in the Document Upload step.")
    else:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("Claude AI Analysis")
            
            if st.button("üöÄ Start AI Analysis", type="primary", disabled=not st.session_state.uploaded_files):
                # Process each document
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                total_files = len(st.session_state.uploaded_files)
                
                for i, (filename, doc_info) in enumerate(st.session_state.uploaded_files.items()):
                    status_text.text(f"Analyzing: {filename}")
                    
                    # Analyze with Claude AI
                    analysis = analyze_document_with_ai(
                        doc_info['content'], 
                        filename, 
                        doc_info['type']
                    )
                    
                    st.session_state.analysis_results[filename] = analysis
                    
                    progress_bar.progress((i + 1) / total_files)
                
                status_text.text("‚úÖ Analysis complete!")
                st.success("üéâ AI analysis completed for all documents!")
        
        with col2:
            st.subheader("üìä Analysis Progress")
            
            if st.session_state.analysis_results:
                analyzed_count = len(st.session_state.analysis_results)
                total_count = len(st.session_state.uploaded_files)
                
                st.metric("Documents Analyzed", f"{analyzed_count}/{total_count}")
                
                # Average confidence score
                avg_confidence = np.mean([doc.get('confidence_score', 0) for doc in st.session_state.analysis_results.values()])
                st.metric("Average Confidence", f"{avg_confidence*100:.1f}%")
                
                # Issues found
                total_issues = sum(len(doc.get('issues_found', [])) for doc in st.session_state.analysis_results.values())
                st.metric("Issues Identified", total_issues)
        
        # Display analysis results
        if st.session_state.analysis_results:
            st.subheader("üìã Analysis Results")
            
            for filename, analysis in st.session_state.analysis_results.items():
                with st.expander(f"üìÑ {filename} - {analysis['document_type']}"):
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**Extracted Data:**")
                        if analysis['extracted_data']:
                            for key, value in analysis['extracted_data'].items():
                                st.write(f"‚Ä¢ {key.replace('_', ' ').title()}: {value}")
                        else:
                            st.write("No structured data extracted")
                    
                    with col2:
                        st.write("**Compliance Items:**")
                        for item in analysis['compliance_items']:
                            st.write(f"‚úÖ {item}")
                        
                        if analysis['issues_found']:
                            st.write("**Issues Found:**")
                            for issue in analysis['issues_found']:
                                st.write(f"‚ö†Ô∏è {issue}")
                    
                    st.write(f"**Confidence Score:** {analysis['confidence_score']*100:.1f}%")

elif workflow_step == "‚úÖ Compliance Check":
    st.header("‚úÖ Due Diligence Compliance Check")
    
    if not st.session_state.analysis_results:
        st.warning("‚ö†Ô∏è Please complete AI analysis first.")
    else:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üìã Due Diligence Checklist")
            
            if st.button("üîç Run Compliance Check", type="primary"):
                # Process compliance checklist
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                total_items = sum(len(items) for section in DD_CHECKLIST.values() for items in section.values())
                current_item = 0
                
                for section_name, subsections in DD_CHECKLIST.items():
                    for subsection_name, items in subsections.items():
                        for item in items:
                            status_text.text(f"Checking: {item[:50]}...")
                            
                            # Check compliance
                            compliance_result = check_compliance_item(item, st.session_state.analysis_results)
                            st.session_state.checklist_status[item] = compliance_result
                            
                            current_item += 1
                            progress_bar.progress(current_item / total_items)
                
                status_text.text("‚úÖ Compliance check complete!")
                st.success("üéâ Due diligence compliance check completed!")
            
            # Display checklist results
            if st.session_state.checklist_status:
                st.subheader("üìä Compliance Results")
                
                for section_name, subsections in DD_CHECKLIST.items():
                    with st.expander(f"üìë {section_name}"):
                        for subsection_name, items in subsections.items():
                            st.write(f"**{subsection_name}**")
                            
                            for item in items:
                                status = st.session_state.checklist_status.get(item, (None, "Not checked"))
                                
                                if status[0] is True:
                                    st.write(f"‚úÖ {item}")
                                elif status[0] is False:
                                    st.write(f"‚ùå {item}")
                                    st.write(f"   üí° {status[1]}")
                                    
                                    # Add to discrepancies
                                    discrepancy = {
                                        'type': 'Compliance Issue',
                                        'description': item,
                                        'severity': 'High',
                                        'recommendation': status[1]
                                    }
                                    
                                    if discrepancy not in st.session_state.discrepancies:
                                        st.session_state.discrepancies.append(discrepancy)
                                else:
                                    st.write(f"‚è≥ {item}")
                                    st.write(f"   üìù {status[1]}")
        
        with col2:
            st.subheader("üìä Compliance Summary")
            
            if st.session_state.checklist_status:
                # Calculate compliance stats
                total_items = len(st.session_state.checklist_status)
                passed_items = sum(1 for status in st.session_state.checklist_status.values() if status[0] is True)
                failed_items = sum(1 for status in st.session_state.checklist_status.values() if status[0] is False)
                pending_items = total_items - passed_items - failed_items
                
                st.metric("Total Items", total_items)
                st.metric("‚úÖ Passed", passed_items)
                st.metric("‚ùå Failed", failed_items)
                st.metric("‚è≥ Pending Review", pending_items)
                
                # Compliance pie chart
                if total_items > 0:
                    fig = px.pie(
                        values=[passed_items, failed_items, pending_items],
                        names=['Passed', 'Failed', 'Pending'],
                        title="Compliance Status",
                        color_discrete_map={'Passed': 'green', 'Failed': 'red', 'Pending': 'orange'}
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Compliance score
                compliance_score = (passed_items / total_items * 100) if total_items > 0 else 0
                st.metric("Compliance Score", f"{compliance_score:.1f}%")

elif workflow_step == "üìä Tie-Out Report":
    st.header("üìä Cap Table Tie-Out Report")
    
    if not st.session_state.checklist_status:
        st.warning("‚ö†Ô∏è Please complete compliance check first.")
    else:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üìã Final Report")
            
            # Generate report
            report_content = generate_tie_out_report(
                st.session_state.analysis_results,
                st.session_state.checklist_status,
                st.session_state.discrepancies
            )
            
            # Display report
            st.markdown(report_content)
            
            # Download report
            st.download_button(
                label="üì• Download Report",
                data=report_content,
                file_name=f"cap_table_tieout_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown"
            )
        
        with col2:
            st.subheader("üéØ Key Metrics")
            
            # Final metrics
            total_docs = len(st.session_state.analysis_results)
            total_compliance_items = len(st.session_state.checklist_status)
            total_discrepancies = len(st.session_state.discrepancies)
            
            st.metric("Documents Analyzed", total_docs)
            st.metric("Compliance Items Checked", total_compliance_items)
            st.metric("Discrepancies Found", total_discrepancies)
            
            # Overall risk assessment
            if total_discrepancies == 0:
                risk_level = "üü¢ Low Risk"
                risk_color = "green"
            elif total_discrepancies <= 3:
                risk_level = "üü° Medium Risk"
                risk_color = "orange"
            else:
                risk_level = "üî¥ High Risk"
                risk_color = "red"
            
            st.markdown(f"**Overall Risk Assessment:** {risk_level}")
            
            # Recommendations
            st.subheader("üí° Next Steps")
            if total_discrepancies > 0:
                st.write("1. Address identified discrepancies")
                st.write("2. Obtain missing documentation")
                st.write("3. Update cap table records")
                st.write("4. Consult legal counsel if needed")
            else:
                st.write("‚úÖ Cap table appears to be in good order")
                st.write("‚úÖ Ready for transaction closing")
            
            # Sample cap table display
            st.subheader("üìä Sample Cap Table")
            if st.session_state.current_cap_table is None:
                st.session_state.current_cap_table = create_sample_cap_table()
            
            st.dataframe(st.session_state.current_cap_table, use_container_width=True)

# Footer
st.markdown("---")
st.markdown("**Cap Table Tie-Out Analysis Tool** | Powered by Claude AI | Built with Streamlit")
st.markdown("*This tool provides automated analysis assistance. Always consult qualified legal counsel for final review.*")
